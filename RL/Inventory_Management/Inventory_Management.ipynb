{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T13:49:50.101343Z",
     "start_time": "2020-01-22T13:49:50.093760Z"
    }
   },
   "outputs": [],
   "source": [
    "# =====> MDP file\n",
    "\n",
    "\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "capacity = 50 # max capaciy of warhouse\n",
    "transport_cost = 5  # costant part of order cost, can be fuel cost\n",
    "variable_cost = 4  # variable part of order cost\n",
    "holding_cost = 0.0025 # holiding cost of inventory\n",
    "selling_price = 4.2 # including the 5% profit\n",
    "return_cost = transport_cost\n",
    "\n",
    "\n",
    "lamda_mon = 16\n",
    "lamda_tue = 31\n",
    "lamda_wed = 15\n",
    "lamda_thu = 32\n",
    "lamda_fri = 30\n",
    "lamda_sat = 8\n",
    "lamda_sun = 42\n",
    "\n",
    "day_mapping = {\n",
    "    0: lamda_mon,\n",
    "    1: lamda_tue,\n",
    "    2: lamda_wed,\n",
    "    3: lamda_thu,\n",
    "    4: lamda_fri,\n",
    "    5: lamda_sat,\n",
    "    6: lamda_sun\n",
    "}\n",
    "daily_mapping = {0: 'mon', 1: 'tue', 2: 'wed', 3: 'thu', 4: 'fir', 5:  'sat', 6: 'sun'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T13:49:50.747654Z",
     "start_time": "2020-01-22T13:49:50.740076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(3, 5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T14:02:00.569641Z",
     "start_time": "2020-01-22T14:02:00.551224Z"
    }
   },
   "outputs": [],
   "source": [
    "class InventoryEnv():\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(capacity + 1)\n",
    "        self.inventory = np.random.choice(np.arange(0, capacity + 1))\n",
    "        self.day = np.random.choice((0, 1, 2, 3, 4, 5, 6))\n",
    "        self.state = (self.inventory, self.day)\n",
    "        self.reset()\n",
    "\n",
    "    def demand(self, day):\n",
    "        return np.random.poisson(day_mapping.get(day))\n",
    "\n",
    "    def transition(self, current_state, action_taken, demand):\n",
    "        stock_after_sales = max(current_state[0] - demand, 0)\n",
    "        # ===> stock by EOD after sales\n",
    "        stock_eod = min(stock_after_sales + action_taken, capacity)\n",
    "\n",
    "        if (current_state[1] < 6):\n",
    "            next_day = current_state[1] + 1\n",
    "        else:\n",
    "            next_day = 0\n",
    "\n",
    "        return (stock_eod, next_day)\n",
    "\n",
    "    def reward(self, inventory, action_taken, demand):\n",
    "        # ===> expected income\n",
    "        expected_income = selling_price * min(inventory, demand)\n",
    "        # ===> order cost\n",
    "        order_cost = (transport_cost *\n",
    "                      (action_taken > 0)) + variable_cost * action_taken\n",
    "        # ===> holding cost\n",
    "        total_holding_cost = holding_cost * inventory\n",
    "        # ===> opportunity cost\n",
    "\n",
    "        profit = selling_price - variable_cost\n",
    "        opportunity_cost = profit * (demand - inventory) * (demand > inventory)\n",
    "        # ===> return cost\n",
    "        stock_after_sales = inventory - demand\n",
    "        stock_arrived = action_taken\n",
    "        total_return_cost = return_cost * (stock_after_sales + stock_arrived >\n",
    "                                     capacity)\n",
    "        # ===> money back\n",
    "\n",
    "        money_back = variable_cost * (\n",
    "            stock_after_sales + stock_arrived -\n",
    "            capacity) * (stock_after_sales + stock_arrived > capacity)\n",
    "\n",
    "        reward = expected_income - order_cost - holding_cost - opportunity_cost - total_return_cost + money_back\n",
    "        return reward\n",
    "\n",
    "    def initial_step(self, current_state, action):\n",
    "        assert self.action_space.contains(action)     #to check that action is a discrete value less than m\n",
    "        if current_state[1]<6:\n",
    "            demand = self.demand(current_state[1]+1)    \n",
    "        else:\n",
    "            demand = self.demand(0)        \n",
    "        next_state = self.transition(current_state, action, demand)       #next_state\n",
    "        return next_state\n",
    "        \n",
    "    def step(self, current_state, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        if (current_state[1] < 6):\n",
    "            next_day_demand = self.demand(current_state[1] + 1)\n",
    "        else:\n",
    "            next_day_demand = self.demand(0)\n",
    "        \n",
    "        next_state = self.transition(current_state, action, next_day_demand)\n",
    "        reward = self.reward(current_state[0], action, next_day_demand)\n",
    "        \n",
    "        return next_state, reward\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning using MDP above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T14:02:01.331856Z",
     "start_time": "2020-01-22T14:02:01.328501Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T14:02:01.684486Z",
     "start_time": "2020-01-22T14:02:01.667901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "7\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "Q_dict = collections.defaultdict(dict)\n",
    "States_track = collections.defaultdict(dict)\n",
    "rewards_tracked = {(15,0):[],(25,1):[], (20,2): [], (30,3):[], (35,4):[], (10,5):[],(50,6):[]}\n",
    "\n",
    "\n",
    "print(len(Q_dict))\n",
    "print(len(rewards_tracked))\n",
    "print(len(States_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T14:02:01.869190Z",
     "start_time": "2020-01-22T14:02:01.696058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "with open('Results_15million/Policy.pkl', 'rb') as handle:\n",
    "    Q_dict = pickle.load(handle)\n",
    "\n",
    "with open('Results_15million/Rewards.pkl', 'rb') as handle:\n",
    "    rewards_tracked = pickle.load(handle)\n",
    "    \n",
    "with open('Results_15million/States_tracked.pkl', 'rb') as handle:\n",
    "    States_track = pickle.load(handle)\n",
    "\n",
    "    \n",
    "print(len(Q_dict))\n",
    "print(len(rewards_tracked))\n",
    "print(len(States_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T14:02:02.243808Z",
     "start_time": "2020-01-22T14:02:02.237648Z"
    }
   },
   "outputs": [],
   "source": [
    "def valid_actions(state):\n",
    "    state = state[0]\n",
    "    valid_Actions = []\n",
    "    for ix in range(0, capacity+1):\n",
    "        valid_Actions.append(ix)\n",
    "    return valid_Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T14:02:02.569344Z",
     "start_time": "2020-01-22T14:02:02.564423Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_to_dict(state,valid_act):\n",
    "    if state not in Q_dict.keys():\n",
    "        Q_dict[state] = {}\n",
    "        for action in valid_act:\n",
    "            Q_dict[state][action] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T14:02:02.856984Z",
     "start_time": "2020-01-22T14:02:02.853067Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T14:02:03.118840Z",
     "start_time": "2020-01-22T14:02:03.112485Z"
    }
   },
   "outputs": [],
   "source": [
    "def initiali_tracking_states():\n",
    "    # Q values ((current_state_inventory, day), action)\n",
    "    sample_q_values = [((15, 0), 12), ((25, 1), 25), ((20, 2), 10),\n",
    "                       ((30, 3), 30), ((35, 4), 15), ((10, 5), 18),\n",
    "                       ((50, 6), 10)]\n",
    "    for q_value in sample_q_values:\n",
    "        state = q_value[0]\n",
    "        action = q_value[1]\n",
    "        States_track[state][action] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T14:02:03.402388Z",
     "start_time": "2020-01-22T14:02:03.395657Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_tracking_states():\n",
    "    for state in States_track.keys():\n",
    "        for action in States_track[state].keys():\n",
    "            if state in Q_dict and action in Q_dict[state]:\n",
    "                States_track[state][action].append(Q_dict[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T14:02:03.728097Z",
     "start_time": "2020-01-22T14:02:03.719419Z"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, time):\n",
    "    epsilon = - 1/ (1 + np.exp((-time+7500000)/1700000)) + 1\n",
    "    z = np.random.random()\n",
    "    if z > epsilon:\n",
    "        # Exploitation\n",
    "        action = max(Q_dict[state],key=Q_dict[state].get)\n",
    "    else:\n",
    "        # Exploration        \n",
    "        action = np.random.choice((np.arange(0, capacity + 1)))\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T13:38:17.413196Z",
     "start_time": "2020-01-22T13:38:17.410293Z"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T14:02:04.229710Z",
     "start_time": "2020-01-22T14:02:04.224911Z"
    }
   },
   "outputs": [],
   "source": [
    "EPISODES = 15000000\n",
    "STEPS = 2\n",
    "# STEPS = 30                 #for 30 days\n",
    "LR = 0.01                   #learning rate\n",
    "GAMMA = 0.91\n",
    "\n",
    "\n",
    "threshold = 2000      \n",
    "policy_threshold = 30000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T20:23:33.013979Z",
     "start_time": "2020-01-22T14:02:04.627331Z"
    }
   },
   "outputs": [],
   "source": [
    "for episode in range(6393884, EPISODES):\n",
    "    env = InventoryEnv()\n",
    "    initial_state = env.state\n",
    "    curr_state = initial_state\n",
    "\n",
    "    add_to_dict(curr_state, valid_actions(curr_state))\n",
    "    time_step = 0\n",
    "    reward = None\n",
    "    curr_action = np.random.choice(np.arange(0, capacity + 1))\n",
    "\n",
    "    next_state = env.initial_step(curr_state, curr_action)\n",
    "    add_to_dict(next_state, valid_actions(next_state))\n",
    "\n",
    "    curr_state = next_state\n",
    "    total_reward = 0\n",
    "    \n",
    "    while time_step < STEPS:\n",
    "        #  Exploitation vs exploration\n",
    "        curr_action = epsilon_greedy(curr_state, episode)\n",
    "        next_state, reward = env.step(curr_state, curr_action)\n",
    "\n",
    "        add_to_dict(next_state, valid_actions(next_state))\n",
    "\n",
    "        max_next = max(Q_dict[next_state], key=Q_dict[next_state].get)\n",
    "\n",
    "        Q_dict[curr_state][curr_action] += LR * (\n",
    "            (reward + (GAMMA * (Q_dict[next_state][max_next]))) -\n",
    "            Q_dict[curr_state][curr_action])\n",
    "        curr_state = next_state\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        time_step += 1\n",
    "    #TRACKING REWARDS\n",
    "    if initial_state in rewards_tracked:     #storing rewards\n",
    "        rewards_tracked[initial_state].append(total_reward)\n",
    "        #save_obj(rewards_tracked,'Rewards')\n",
    "\n",
    "    if ((episode+1) % threshold) == 0:   #every 2000th episode\n",
    "        save_obj(rewards_tracked,'Rewards')   \n",
    "    \n",
    "    #TRACKING Q-VALUES\n",
    "    if (episode == threshold-1):        #at the 1999th episode\n",
    "        initialise_tracking_states()\n",
    "      \n",
    "    if ((episode+1) % threshold) == 0:   #every 2000th episode\n",
    "        save_tracking_states()\n",
    "        save_obj(States_track,'States_tracked')   \n",
    "    \n",
    "    #SAVING POLICY\n",
    "    if ((episode+1)% policy_threshold ) == 0:  #every 30000th episodes, the Q-dict will be saved\n",
    "        save_obj(Q_dict,'Policy')    \n",
    "\n",
    "save_obj(rewards_tracked,'Rewards')   \n",
    "save_obj(States_track,'States_tracked')   \n",
    "save_obj(Q_dict,'Policy')      \n",
    "# print(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T20:23:37.169291Z",
     "start_time": "2020-01-22T20:23:34.237269Z"
    }
   },
   "outputs": [],
   "source": [
    "save_obj(rewards_tracked,'Rewards')   \n",
    "save_obj(States_track,'States_tracked')   \n",
    "save_obj(Q_dict,'Policy')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T12:56:27.533344Z",
     "start_time": "2020-01-22T12:56:27.524731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_actions((30, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
