{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Defaulted - Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>defaulted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...          0          0          0         0       689         0   \n",
       "1  ...       3272       3455       3261         0      1000      1000   \n",
       "2  ...      14331      14948      15549      1518      1500      1000   \n",
       "3  ...      28314      28959      29547      2000      2019      1200   \n",
       "4  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  defaulted  \n",
       "0         0         0         0          1  \n",
       "1      1000         0      2000          1  \n",
       "2      1000      1000      5000          0  \n",
       "3      1100      1069      1000          0  \n",
       "4      9000       689       679          0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_card_data = pd.read_csv('https://cdn.upgrad.com/UpGrad/temp/3a1a166e-3ee4-47ee-bc4c-d9dda21601e2/credit-card-default.csv')\n",
    "credit_card_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 25 columns):\n",
      "ID           30000 non-null int64\n",
      "LIMIT_BAL    30000 non-null int64\n",
      "SEX          30000 non-null int64\n",
      "EDUCATION    30000 non-null int64\n",
      "MARRIAGE     30000 non-null int64\n",
      "AGE          30000 non-null int64\n",
      "PAY_0        30000 non-null int64\n",
      "PAY_2        30000 non-null int64\n",
      "PAY_3        30000 non-null int64\n",
      "PAY_4        30000 non-null int64\n",
      "PAY_5        30000 non-null int64\n",
      "PAY_6        30000 non-null int64\n",
      "BILL_AMT1    30000 non-null int64\n",
      "BILL_AMT2    30000 non-null int64\n",
      "BILL_AMT3    30000 non-null int64\n",
      "BILL_AMT4    30000 non-null int64\n",
      "BILL_AMT5    30000 non-null int64\n",
      "BILL_AMT6    30000 non-null int64\n",
      "PAY_AMT1     30000 non-null int64\n",
      "PAY_AMT2     30000 non-null int64\n",
      "PAY_AMT3     30000 non-null int64\n",
      "PAY_AMT4     30000 non-null int64\n",
      "PAY_AMT5     30000 non-null int64\n",
      "PAY_AMT6     30000 non-null int64\n",
      "defaulted    30000 non-null int64\n",
      "dtypes: int64(25)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "credit_card_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ID       LIMIT_BAL           SEX     EDUCATION      MARRIAGE  \\\n",
      "count  30000.000000    30000.000000  30000.000000  30000.000000  30000.000000   \n",
      "mean   15000.500000   167484.322667      1.603733      1.853133      1.551867   \n",
      "std     8660.398374   129747.661567      0.489129      0.790349      0.521970   \n",
      "min        1.000000    10000.000000      1.000000      0.000000      0.000000   \n",
      "25%     7500.750000    50000.000000      1.000000      1.000000      1.000000   \n",
      "50%    15000.500000   140000.000000      2.000000      2.000000      2.000000   \n",
      "75%    22500.250000   240000.000000      2.000000      2.000000      2.000000   \n",
      "max    30000.000000  1000000.000000      2.000000      6.000000      3.000000   \n",
      "\n",
      "                AGE         PAY_0         PAY_2         PAY_3         PAY_4  \\\n",
      "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
      "mean      35.485500     -0.016700     -0.133767     -0.166200     -0.220667   \n",
      "std        9.217904      1.123802      1.197186      1.196868      1.169139   \n",
      "min       21.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
      "25%       28.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
      "50%       34.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%       41.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max       79.000000      8.000000      8.000000      8.000000      8.000000   \n",
      "\n",
      "       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
      "count  ...   30000.000000   30000.000000   30000.000000   30000.000000   \n",
      "mean   ...   43262.948967   40311.400967   38871.760400    5663.580500   \n",
      "std    ...   64332.856134   60797.155770   59554.107537   16563.280354   \n",
      "min    ... -170000.000000  -81334.000000 -339603.000000       0.000000   \n",
      "25%    ...    2326.750000    1763.000000    1256.000000    1000.000000   \n",
      "50%    ...   19052.000000   18104.500000   17071.000000    2100.000000   \n",
      "75%    ...   54506.000000   50190.500000   49198.250000    5006.000000   \n",
      "max    ...  891586.000000  927171.000000  961664.000000  873552.000000   \n",
      "\n",
      "           PAY_AMT2      PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\n",
      "count  3.000000e+04   30000.00000   30000.000000   30000.000000   \n",
      "mean   5.921163e+03    5225.68150    4826.076867    4799.387633   \n",
      "std    2.304087e+04   17606.96147   15666.159744   15278.305679   \n",
      "min    0.000000e+00       0.00000       0.000000       0.000000   \n",
      "25%    8.330000e+02     390.00000     296.000000     252.500000   \n",
      "50%    2.009000e+03    1800.00000    1500.000000    1500.000000   \n",
      "75%    5.000000e+03    4505.00000    4013.250000    4031.500000   \n",
      "max    1.684259e+06  896040.00000  621000.000000  426529.000000   \n",
      "\n",
      "            PAY_AMT6     defaulted  \n",
      "count   30000.000000  30000.000000  \n",
      "mean     5215.502567      0.221200  \n",
      "std     17777.465775      0.415062  \n",
      "min         0.000000      0.000000  \n",
      "25%       117.750000      0.000000  \n",
      "50%      1500.000000      0.000000  \n",
      "75%      4000.000000      0.000000  \n",
      "max    528666.000000      1.000000  \n",
      "\n",
      "[8 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "print(credit_card_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0\n",
       "LIMIT_BAL    0\n",
       "SEX          0\n",
       "EDUCATION    0\n",
       "MARRIAGE     0\n",
       "AGE          0\n",
       "PAY_0        0\n",
       "PAY_2        0\n",
       "PAY_3        0\n",
       "PAY_4        0\n",
       "PAY_5        0\n",
       "PAY_6        0\n",
       "BILL_AMT1    0\n",
       "BILL_AMT2    0\n",
       "BILL_AMT3    0\n",
       "BILL_AMT4    0\n",
       "BILL_AMT5    0\n",
       "BILL_AMT6    0\n",
       "PAY_AMT1     0\n",
       "PAY_AMT2     0\n",
       "PAY_AMT3     0\n",
       "PAY_AMT4     0\n",
       "PAY_AMT5     0\n",
       "PAY_AMT6     0\n",
       "defaulted    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_card_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x = credit_card_data.drop(columns=['defaulted'], axis=1)\n",
    "y = credit_card_data['defaulted']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Data is already clean & now lets try to build a model\n",
    "### Model Building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble.forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |         The default value of ``n_estimators`` will change from 10 in\n",
      " |         version 0.20 to 100 in version 0.22.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"gini\")\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, (default=1e-7)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole datset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool (default=False)\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization accuracy.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or     None, optional (default=None)\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  \n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
      " |  ...                              random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      " |              max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      " |              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      " |              min_samples_leaf=1, min_samples_split=2,\n",
      " |              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      " |              oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " |  >>> print(clf.feature_importances_)\n",
      " |  [0.14205973 0.76664038 0.0282433  0.06305659]\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      abc.NewBase\n",
      " |      BaseForest\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest. The\n",
      " |      class probability of a single tree is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |      \n",
      " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rf_model.fit(x_train, y_train)\n",
    "y_pred = rf_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89      7058\n",
      "           1       0.61      0.33      0.43      1942\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      9000\n",
      "   macro avg       0.73      0.63      0.66      9000\n",
      "weighted avg       0.79      0.81      0.79      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8104444444444444\n",
      "[[6661  397]\n",
      " [1309  633]]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning - GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Hyper parameters to tune: max_depth, min_samples_leaf, min_samples_split, max_features, n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GridSearchCV in module sklearn.model_selection._search:\n",
      "\n",
      "class GridSearchCV(BaseSearchCV)\n",
      " |  GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0, pre_dispatch='2*n_jobs', error_score='raise-deprecating', return_train_score='warn')\n",
      " |  \n",
      " |  Exhaustive search over specified parameter values for an estimator.\n",
      " |  \n",
      " |  Important members are fit, predict.\n",
      " |  \n",
      " |  GridSearchCV implements a \"fit\" and a \"score\" method.\n",
      " |  It also implements \"predict\", \"predict_proba\", \"decision_function\",\n",
      " |  \"transform\" and \"inverse_transform\" if they are implemented in the\n",
      " |  estimator used.\n",
      " |  \n",
      " |  The parameters of the estimator used to apply these methods are optimized\n",
      " |  by cross-validated grid-search over a parameter grid.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <grid_search>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : estimator object.\n",
      " |      This is assumed to implement the scikit-learn estimator interface.\n",
      " |      Either estimator needs to provide a ``score`` function,\n",
      " |      or ``scoring`` must be passed.\n",
      " |  \n",
      " |  param_grid : dict or list of dictionaries\n",
      " |      Dictionary with parameters names (string) as keys and lists of\n",
      " |      parameter settings to try as values, or a list of such\n",
      " |      dictionaries, in which case the grids spanned by each dictionary\n",
      " |      in the list are explored. This enables searching over any sequence\n",
      " |      of parameter settings.\n",
      " |  \n",
      " |  scoring : string, callable, list/tuple, dict or None, default: None\n",
      " |      A single string (see :ref:`scoring_parameter`) or a callable\n",
      " |      (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      " |  \n",
      " |      For evaluating multiple metrics, either give a list of (unique) strings\n",
      " |      or a dict with names as keys and callables as values.\n",
      " |  \n",
      " |      NOTE that when using custom scorers, each scorer should return a single\n",
      " |      value. Metric functions returning a list/array of values can be wrapped\n",
      " |      into multiple scorers that return one value each.\n",
      " |  \n",
      " |      See :ref:`multimetric_grid_search` for an example.\n",
      " |  \n",
      " |      If None, the estimator's default scorer (if available) is used.\n",
      " |  \n",
      " |  fit_params : dict, optional\n",
      " |      Parameters to pass to the fit method.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``fit_params`` as a constructor argument was deprecated in version\n",
      " |         0.19 and will be removed in version 0.21. Pass fit parameters to\n",
      " |         the ``fit`` method instead.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      Number of jobs to run in parallel.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  pre_dispatch : int, or string, optional\n",
      " |      Controls the number of jobs that get dispatched during parallel\n",
      " |      execution. Reducing this number can be useful to avoid an\n",
      " |      explosion of memory consumption when more jobs get dispatched\n",
      " |      than CPUs can process. This parameter can be:\n",
      " |  \n",
      " |          - None, in which case all the jobs are immediately\n",
      " |            created and spawned. Use this for lightweight and\n",
      " |            fast-running jobs, to avoid delays due to on-demand\n",
      " |            spawning of the jobs\n",
      " |  \n",
      " |          - An int, giving the exact number of total jobs that are\n",
      " |            spawned\n",
      " |  \n",
      " |          - A string, giving an expression as a function of n_jobs,\n",
      " |            as in '2*n_jobs'\n",
      " |  \n",
      " |  iid : boolean, default='warn'\n",
      " |      If True, return the average score across folds, weighted by the number\n",
      " |      of samples in each test set. In this case, the data is assumed to be\n",
      " |      identically distributed across the folds, and the loss minimized is\n",
      " |      the total loss per sample, and not the mean loss across the folds. If\n",
      " |      False, return the average score across folds. Default is True, but\n",
      " |      will change to False in version 0.21, to correspond to the standard\n",
      " |      definition of cross-validation.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          Parameter ``iid`` will change from True to False by default in\n",
      " |          version 0.22, and will be removed in 0.24.\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, optional\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |  \n",
      " |      - None, to use the default 3-fold cross validation,\n",
      " |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      " |      - :term:`CV splitter`,\n",
      " |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      " |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      " |      other cases, :class:`KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          ``cv`` default value if None will change from 3-fold to 5-fold\n",
      " |          in v0.22.\n",
      " |  \n",
      " |  refit : boolean, or string, default=True\n",
      " |      Refit an estimator using the best found parameters on the whole\n",
      " |      dataset.\n",
      " |  \n",
      " |      For multiple metric evaluation, this needs to be a string denoting the\n",
      " |      scorer is used to find the best parameters for refitting the estimator\n",
      " |      at the end.\n",
      " |  \n",
      " |      The refitted estimator is made available at the ``best_estimator_``\n",
      " |      attribute and permits using ``predict`` directly on this\n",
      " |      ``GridSearchCV`` instance.\n",
      " |  \n",
      " |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      " |      ``best_score_`` and ``best_params_`` will only be available if\n",
      " |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      " |      scorer.\n",
      " |  \n",
      " |      See ``scoring`` parameter to know more about multiple metric\n",
      " |      evaluation.\n",
      " |  \n",
      " |  verbose : integer\n",
      " |      Controls the verbosity: the higher, the more messages.\n",
      " |  \n",
      " |  error_score : 'raise' or numeric\n",
      " |      Value to assign to the score if an error occurs in estimator fitting.\n",
      " |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      " |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      " |      step, which will always raise the error. Default is 'raise' but from\n",
      " |      version 0.22 it will change to np.nan.\n",
      " |  \n",
      " |  return_train_score : boolean, optional\n",
      " |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      " |      scores.\n",
      " |  \n",
      " |      Current default is ``'warn'``, which behaves as ``True`` in addition\n",
      " |      to raising a warning when a training score is looked up.\n",
      " |      That default will be changed to ``False`` in 0.21.\n",
      " |      Computing training scores is used to get insights on how different\n",
      " |      parameter settings impact the overfitting/underfitting trade-off.\n",
      " |      However computing the scores on the training set can be computationally\n",
      " |      expensive and is not strictly required to select the parameters that\n",
      " |      yield the best generalization performance.\n",
      " |  \n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import svm, datasets\n",
      " |  >>> from sklearn.model_selection import GridSearchCV\n",
      " |  >>> iris = datasets.load_iris()\n",
      " |  >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      " |  >>> svc = svm.SVC(gamma=\"scale\")\n",
      " |  >>> clf = GridSearchCV(svc, parameters, cv=5)\n",
      " |  >>> clf.fit(iris.data, iris.target)\n",
      " |  ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      " |  GridSearchCV(cv=5, error_score=...,\n",
      " |         estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,\n",
      " |                       decision_function_shape='ovr', degree=..., gamma=...,\n",
      " |                       kernel='rbf', max_iter=-1, probability=False,\n",
      " |                       random_state=None, shrinking=True, tol=...,\n",
      " |                       verbose=False),\n",
      " |         fit_params=None, iid=..., n_jobs=None,\n",
      " |         param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,\n",
      " |         scoring=..., verbose=...)\n",
      " |  >>> sorted(clf.cv_results_.keys())\n",
      " |  ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      " |  ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
      " |   'mean_train_score', 'param_C', 'param_kernel', 'params',...\n",
      " |   'rank_test_score', 'split0_test_score',...\n",
      " |   'split0_train_score', 'split1_test_score', 'split1_train_score',...\n",
      " |   'split2_test_score', 'split2_train_score',...\n",
      " |   'std_fit_time', 'std_score_time', 'std_test_score', 'std_train_score'...]\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cv_results_ : dict of numpy (masked) ndarrays\n",
      " |      A dict with keys as column headers and values as columns, that can be\n",
      " |      imported into a pandas ``DataFrame``.\n",
      " |  \n",
      " |      For instance the below given table\n",
      " |  \n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
      " |      +============+===========+============+=================+===+=========+\n",
      " |      |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |  \n",
      " |      will be represented by a ``cv_results_`` dict of::\n",
      " |  \n",
      " |          {\n",
      " |          'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
      " |                                       mask = [False False False False]...)\n",
      " |          'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
      " |                                      mask = [ True  True False False]...),\n",
      " |          'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
      " |                                       mask = [False False  True  True]...),\n",
      " |          'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
      " |          'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
      " |          'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
      " |          'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
      " |          'rank_test_score'    : [2, 4, 3, 1],\n",
      " |          'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
      " |          'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
      " |          'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
      " |          'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
      " |          'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
      " |          'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
      " |          'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
      " |          'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
      " |          'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
      " |          }\n",
      " |  \n",
      " |      NOTE\n",
      " |  \n",
      " |      The key ``'params'`` is used to store a list of parameter\n",
      " |      settings dicts for all the parameter candidates.\n",
      " |  \n",
      " |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      " |      ``std_score_time`` are all in seconds.\n",
      " |  \n",
      " |      For multi-metric evaluation, the scores for all the scorers are\n",
      " |      available in the ``cv_results_`` dict at the keys ending with that\n",
      " |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      " |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      " |  \n",
      " |  best_estimator_ : estimator or dict\n",
      " |      Estimator that was chosen by the search, i.e. estimator\n",
      " |      which gave highest score (or smallest loss if specified)\n",
      " |      on the left out data. Not available if ``refit=False``.\n",
      " |  \n",
      " |      See ``refit`` parameter for more information on allowed values.\n",
      " |  \n",
      " |  best_score_ : float\n",
      " |      Mean cross-validated score of the best_estimator\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  best_params_ : dict\n",
      " |      Parameter setting that gave the best results on the hold out data.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  best_index_ : int\n",
      " |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      " |      candidate parameter setting.\n",
      " |  \n",
      " |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      " |      the parameter setting for the best model, that gives the highest\n",
      " |      mean score (``search.best_score_``).\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  scorer_ : function or a dict\n",
      " |      Scorer function used on the held out data to choose the best\n",
      " |      parameters for the model.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute holds the validated\n",
      " |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      " |  \n",
      " |  n_splits_ : int\n",
      " |      The number of cross-validation splits (folds/iterations).\n",
      " |  \n",
      " |  refit_time_ : float\n",
      " |      Seconds used for refitting the best model on the whole dataset.\n",
      " |  \n",
      " |      This is present only if ``refit`` is not False.\n",
      " |  \n",
      " |  Notes\n",
      " |  ------\n",
      " |  The parameters selected are those that maximize the score of the left out\n",
      " |  data, unless an explicit score is passed in which case it is used instead.\n",
      " |  \n",
      " |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      " |  point in the grid (and not `n_jobs` times). This is done for efficiency\n",
      " |  reasons if individual jobs take very little time, but may raise errors if\n",
      " |  the dataset is large and not enough memory is available.  A workaround in\n",
      " |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      " |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      " |  n_jobs`.\n",
      " |  \n",
      " |  See Also\n",
      " |  ---------\n",
      " |  :class:`ParameterGrid`:\n",
      " |      generates all the combinations of a hyperparameter grid.\n",
      " |  \n",
      " |  :func:`sklearn.model_selection.train_test_split`:\n",
      " |      utility function to split the data into a development set usable\n",
      " |      for fitting a GridSearchCV instance and an evaluation set for\n",
      " |      its final evaluation.\n",
      " |  \n",
      " |  :func:`sklearn.metrics.make_scorer`:\n",
      " |      Make a scorer from a performance metric or loss function.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GridSearchCV\n",
      " |      BaseSearchCV\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, param_grid, scoring=None, fit_params=None, n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0, pre_dispatch='2*n_jobs', error_score='raise-deprecating', return_train_score='warn')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSearchCV:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Call decision_function on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``decision_function``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  fit(self, X, y=None, groups=None, **fit_params)\n",
      " |      Run fit with all sets of parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of the estimator\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Call inverse_transform on the estimator with the best found params.\n",
      " |      \n",
      " |      Only available if the underlying estimator implements\n",
      " |      ``inverse_transform`` and ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      Xt : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Call predict on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Call predict_log_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_log_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Call predict_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Returns the score on the given data, if the estimator has been refit.\n",
      " |      \n",
      " |      This uses the score defined by ``scoring`` where provided, and the\n",
      " |      ``best_estimator_.score`` method otherwise.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |          Input data, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Call transform on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if the underlying estimator supports ``transform`` and\n",
      " |      ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseSearchCV:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   35.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=101, shuffle=True),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_depth': [3, 4, 5, 6, 7, 8]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===> Hyper parameters\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=101)\n",
    "params = {  'max_depth': [3, 4, 5, 6, 7, 8]  }\n",
    "grid_cv1 = GridSearchCV(rf_model, param_grid=params, scoring='accuracy', n_jobs = -1, cv=folds, verbose=1, return_train_score=True)\n",
    "grid_cv1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.155591</td>\n",
       "      <td>0.012708</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 3}</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.804762</td>\n",
       "      <td>0.813571</td>\n",
       "      <td>0.804048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806286</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>6</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.808095</td>\n",
       "      <td>0.810357</td>\n",
       "      <td>0.809048</td>\n",
       "      <td>0.800714</td>\n",
       "      <td>0.806810</td>\n",
       "      <td>0.003387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.158683</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 4}</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.802619</td>\n",
       "      <td>0.811905</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811952</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>4</td>\n",
       "      <td>0.813393</td>\n",
       "      <td>0.807619</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.814405</td>\n",
       "      <td>0.818274</td>\n",
       "      <td>0.813024</td>\n",
       "      <td>0.003503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196276</td>\n",
       "      <td>0.005530</td>\n",
       "      <td>0.007076</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 5}</td>\n",
       "      <td>0.814524</td>\n",
       "      <td>0.810238</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.815952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813571</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>3</td>\n",
       "      <td>0.817798</td>\n",
       "      <td>0.815060</td>\n",
       "      <td>0.823810</td>\n",
       "      <td>0.822381</td>\n",
       "      <td>0.812440</td>\n",
       "      <td>0.818298</td>\n",
       "      <td>0.004292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.219155</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.007567</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 6}</td>\n",
       "      <td>0.810714</td>\n",
       "      <td>0.807381</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811286</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>5</td>\n",
       "      <td>0.817202</td>\n",
       "      <td>0.822440</td>\n",
       "      <td>0.826369</td>\n",
       "      <td>0.822202</td>\n",
       "      <td>0.821726</td>\n",
       "      <td>0.821988</td>\n",
       "      <td>0.002913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.249175</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>0.008770</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 7}</td>\n",
       "      <td>0.814524</td>\n",
       "      <td>0.808571</td>\n",
       "      <td>0.816429</td>\n",
       "      <td>0.814762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815286</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824345</td>\n",
       "      <td>0.832798</td>\n",
       "      <td>0.826726</td>\n",
       "      <td>0.830357</td>\n",
       "      <td>0.830119</td>\n",
       "      <td>0.828869</td>\n",
       "      <td>0.002975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.253156</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>8</td>\n",
       "      <td>{'max_depth': 8}</td>\n",
       "      <td>0.815476</td>\n",
       "      <td>0.807857</td>\n",
       "      <td>0.815714</td>\n",
       "      <td>0.814762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.004246</td>\n",
       "      <td>2</td>\n",
       "      <td>0.840893</td>\n",
       "      <td>0.840298</td>\n",
       "      <td>0.837857</td>\n",
       "      <td>0.839821</td>\n",
       "      <td>0.840179</td>\n",
       "      <td>0.839810</td>\n",
       "      <td>0.001035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.155591      0.012708         0.008760        0.001680   \n",
       "1       0.158683      0.004683         0.005481        0.001066   \n",
       "2       0.196276      0.005530         0.007076        0.000698   \n",
       "3       0.219155      0.005626         0.007567        0.000979   \n",
       "4       0.249175      0.012852         0.008770        0.001835   \n",
       "5       0.253156      0.036596         0.006766        0.001403   \n",
       "\n",
       "  param_max_depth            params  split0_test_score  split1_test_score  \\\n",
       "0               3  {'max_depth': 3}           0.805000           0.804762   \n",
       "1               4  {'max_depth': 4}           0.815000           0.802619   \n",
       "2               5  {'max_depth': 5}           0.814524           0.810238   \n",
       "3               6  {'max_depth': 6}           0.810714           0.807381   \n",
       "4               7  {'max_depth': 7}           0.814524           0.808571   \n",
       "5               8  {'max_depth': 8}           0.815476           0.807857   \n",
       "\n",
       "   split2_test_score  split3_test_score  ...  mean_test_score  std_test_score  \\\n",
       "0           0.813571           0.804048  ...         0.806286        0.003663   \n",
       "1           0.811905           0.808333  ...         0.811952        0.006458   \n",
       "2           0.816667           0.815952  ...         0.813571        0.002715   \n",
       "3           0.819286           0.808333  ...         0.811286        0.004210   \n",
       "4           0.816429           0.814762  ...         0.815286        0.004342   \n",
       "5           0.815714           0.814762  ...         0.815000        0.004246   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                6            0.805833            0.808095   \n",
       "1                4            0.813393            0.807619   \n",
       "2                3            0.817798            0.815060   \n",
       "3                5            0.817202            0.822440   \n",
       "4                1            0.824345            0.832798   \n",
       "5                2            0.840893            0.840298   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            0.810357            0.809048            0.800714   \n",
       "1            0.811429            0.814405            0.818274   \n",
       "2            0.823810            0.822381            0.812440   \n",
       "3            0.826369            0.822202            0.821726   \n",
       "4            0.826726            0.830357            0.830119   \n",
       "5            0.837857            0.839821            0.840179   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0          0.806810         0.003387  \n",
       "1          0.813024         0.003503  \n",
       "2          0.818298         0.004292  \n",
       "3          0.821988         0.002913  \n",
       "4          0.828869         0.002975  \n",
       "5          0.839810         0.001035  \n",
       "\n",
       "[6 rows x 21 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_result = pd.DataFrame(grid_cv1.cv_results_)\n",
    "_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(_result['param_max_depth'], _result['mean_train_score'], label='Training Accuracy')\n",
    "plt.plot(_result['param_max_depth'], _result['mean_test_score'], label='Test Accuracy')\n",
    "plt.xlabel('Max depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Training & Test accuracy is drastically decreased after 4th level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_samples_split: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   10.3s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.722993</td>\n",
       "      <td>0.015174</td>\n",
       "      <td>0.021888</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_split': 5}</td>\n",
       "      <td>0.806905</td>\n",
       "      <td>0.807619</td>\n",
       "      <td>0.804524</td>\n",
       "      <td>0.792381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>10</td>\n",
       "      <td>0.963512</td>\n",
       "      <td>0.962619</td>\n",
       "      <td>0.964107</td>\n",
       "      <td>0.963929</td>\n",
       "      <td>0.962976</td>\n",
       "      <td>0.963429</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.642912</td>\n",
       "      <td>0.027499</td>\n",
       "      <td>0.016130</td>\n",
       "      <td>0.006063</td>\n",
       "      <td>35</td>\n",
       "      <td>{'min_samples_split': 35}</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.818095</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814619</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>8</td>\n",
       "      <td>0.857202</td>\n",
       "      <td>0.857857</td>\n",
       "      <td>0.857024</td>\n",
       "      <td>0.863155</td>\n",
       "      <td>0.859048</td>\n",
       "      <td>0.858857</td>\n",
       "      <td>0.002263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.546089</td>\n",
       "      <td>0.043298</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>65</td>\n",
       "      <td>{'min_samples_split': 65}</td>\n",
       "      <td>0.815476</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.818571</td>\n",
       "      <td>0.807381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814429</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>9</td>\n",
       "      <td>0.841786</td>\n",
       "      <td>0.839524</td>\n",
       "      <td>0.839940</td>\n",
       "      <td>0.844107</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.841821</td>\n",
       "      <td>0.001885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.565662</td>\n",
       "      <td>0.058323</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>95</td>\n",
       "      <td>{'min_samples_split': 95}</td>\n",
       "      <td>0.820476</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.806190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815905</td>\n",
       "      <td>0.005625</td>\n",
       "      <td>6</td>\n",
       "      <td>0.831726</td>\n",
       "      <td>0.831726</td>\n",
       "      <td>0.833810</td>\n",
       "      <td>0.833571</td>\n",
       "      <td>0.834107</td>\n",
       "      <td>0.832988</td>\n",
       "      <td>0.001044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.612696</td>\n",
       "      <td>0.099335</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>125</td>\n",
       "      <td>{'min_samples_split': 125}</td>\n",
       "      <td>0.820714</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.819524</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816571</td>\n",
       "      <td>0.005571</td>\n",
       "      <td>3</td>\n",
       "      <td>0.827500</td>\n",
       "      <td>0.829583</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.828929</td>\n",
       "      <td>0.830476</td>\n",
       "      <td>0.828631</td>\n",
       "      <td>0.001381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.459934</td>\n",
       "      <td>0.011711</td>\n",
       "      <td>0.011049</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>155</td>\n",
       "      <td>{'min_samples_split': 155}</td>\n",
       "      <td>0.820952</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.821905</td>\n",
       "      <td>0.805238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816286</td>\n",
       "      <td>0.006155</td>\n",
       "      <td>5</td>\n",
       "      <td>0.826310</td>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.824167</td>\n",
       "      <td>0.828512</td>\n",
       "      <td>0.828869</td>\n",
       "      <td>0.826536</td>\n",
       "      <td>0.001895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.422790</td>\n",
       "      <td>0.009993</td>\n",
       "      <td>0.010409</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>185</td>\n",
       "      <td>{'min_samples_split': 185}</td>\n",
       "      <td>0.819524</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.823571</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816810</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>2</td>\n",
       "      <td>0.823512</td>\n",
       "      <td>0.823214</td>\n",
       "      <td>0.824167</td>\n",
       "      <td>0.827500</td>\n",
       "      <td>0.824524</td>\n",
       "      <td>0.824583</td>\n",
       "      <td>0.001530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.417694</td>\n",
       "      <td>0.023922</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>215</td>\n",
       "      <td>{'min_samples_split': 215}</td>\n",
       "      <td>0.821190</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816476</td>\n",
       "      <td>0.005468</td>\n",
       "      <td>4</td>\n",
       "      <td>0.822321</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.825536</td>\n",
       "      <td>0.822798</td>\n",
       "      <td>0.823143</td>\n",
       "      <td>0.001231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.414111</td>\n",
       "      <td>0.012632</td>\n",
       "      <td>0.008828</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>245</td>\n",
       "      <td>{'min_samples_split': 245}</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.820714</td>\n",
       "      <td>0.820476</td>\n",
       "      <td>0.807381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817619</td>\n",
       "      <td>0.005494</td>\n",
       "      <td>1</td>\n",
       "      <td>0.821905</td>\n",
       "      <td>0.821845</td>\n",
       "      <td>0.821012</td>\n",
       "      <td>0.826310</td>\n",
       "      <td>0.823095</td>\n",
       "      <td>0.822833</td>\n",
       "      <td>0.001861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.358730</td>\n",
       "      <td>0.050037</td>\n",
       "      <td>0.007232</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>275</td>\n",
       "      <td>{'min_samples_split': 275}</td>\n",
       "      <td>0.820238</td>\n",
       "      <td>0.812619</td>\n",
       "      <td>0.821190</td>\n",
       "      <td>0.806905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815333</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>7</td>\n",
       "      <td>0.821190</td>\n",
       "      <td>0.816548</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.823274</td>\n",
       "      <td>0.822619</td>\n",
       "      <td>0.821298</td>\n",
       "      <td>0.002476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.722993      0.015174         0.021888        0.004387   \n",
       "1       0.642912      0.027499         0.016130        0.006063   \n",
       "2       0.546089      0.043298         0.011510        0.001798   \n",
       "3       0.565662      0.058323         0.014866        0.003724   \n",
       "4       0.612696      0.099335         0.012795        0.002544   \n",
       "5       0.459934      0.011711         0.011049        0.002712   \n",
       "6       0.422790      0.009993         0.010409        0.000465   \n",
       "7       0.417694      0.023922         0.009476        0.000739   \n",
       "8       0.414111      0.012632         0.008828        0.000695   \n",
       "9       0.358730      0.050037         0.007232        0.000942   \n",
       "\n",
       "  param_min_samples_split                      params  split0_test_score  \\\n",
       "0                       5    {'min_samples_split': 5}           0.806905   \n",
       "1                      35   {'min_samples_split': 35}           0.817143   \n",
       "2                      65   {'min_samples_split': 65}           0.815476   \n",
       "3                      95   {'min_samples_split': 95}           0.820476   \n",
       "4                     125  {'min_samples_split': 125}           0.820714   \n",
       "5                     155  {'min_samples_split': 155}           0.820952   \n",
       "6                     185  {'min_samples_split': 185}           0.819524   \n",
       "7                     215  {'min_samples_split': 215}           0.821190   \n",
       "8                     245  {'min_samples_split': 245}           0.822857   \n",
       "9                     275  {'min_samples_split': 275}           0.820238   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           0.807619           0.804524           0.792381  ...   \n",
       "1           0.818095           0.817381           0.809524  ...   \n",
       "2           0.817143           0.818571           0.807381  ...   \n",
       "3           0.816667           0.822143           0.806190  ...   \n",
       "4           0.821667           0.819524           0.806667  ...   \n",
       "5           0.819286           0.821905           0.805238  ...   \n",
       "6           0.822143           0.823571           0.806667  ...   \n",
       "7           0.817857           0.821667           0.806667  ...   \n",
       "8           0.820714           0.820476           0.807381  ...   \n",
       "9           0.812619           0.821190           0.806905  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.802857        0.005506               10            0.963512   \n",
       "1         0.814619        0.003619                8            0.857202   \n",
       "2         0.814429        0.003900                9            0.841786   \n",
       "3         0.815905        0.005625                6            0.831726   \n",
       "4         0.816571        0.005571                3            0.827500   \n",
       "5         0.816286        0.006155                5            0.826310   \n",
       "6         0.816810        0.006422                2            0.823512   \n",
       "7         0.816476        0.005468                4            0.822321   \n",
       "8         0.817619        0.005494                1            0.821905   \n",
       "9         0.815333        0.005233                7            0.821190   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            0.962619            0.964107            0.963929   \n",
       "1            0.857857            0.857024            0.863155   \n",
       "2            0.839524            0.839940            0.844107   \n",
       "3            0.831726            0.833810            0.833571   \n",
       "4            0.829583            0.826667            0.828929   \n",
       "5            0.824821            0.824167            0.828512   \n",
       "6            0.823214            0.824167            0.827500   \n",
       "7            0.822143            0.822917            0.825536   \n",
       "8            0.821845            0.821012            0.826310   \n",
       "9            0.816548            0.822857            0.823274   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0            0.962976          0.963429         0.000562  \n",
       "1            0.859048          0.858857         0.002263  \n",
       "2            0.843750          0.841821         0.001885  \n",
       "3            0.834107          0.832988         0.001044  \n",
       "4            0.830476          0.828631         0.001381  \n",
       "5            0.828869          0.826536         0.001895  \n",
       "6            0.824524          0.824583         0.001530  \n",
       "7            0.822798          0.823143         0.001231  \n",
       "8            0.823095          0.822833         0.001861  \n",
       "9            0.822619          0.821298         0.002476  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, random_state=True, shuffle=True)\n",
    "params = {'min_samples_split': range(5, 300, 30)}\n",
    "gs_model2 = GridSearchCV(rf_model,\n",
    "             param_grid=params,\n",
    "             scoring='accuracy',\n",
    "             n_jobs=-1,\n",
    "             cv=folds,\n",
    "             verbose=1,\n",
    "             return_train_score=True)\n",
    "gs_model2.fit(x_train, y_train)\n",
    "results = pd.DataFrame(gs_model2.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(results['param_min_samples_split'], results['mean_train_score'], label='Training Accuracy')\n",
    "plt.plot(results['param_min_samples_split'], results['mean_test_score'], label='Test Accuracy')\n",
    "plt.xlabel('Training Accuracy')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_samples_leaf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.850318</td>\n",
       "      <td>0.030533</td>\n",
       "      <td>0.029697</td>\n",
       "      <td>0.012241</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_leaf': 5}</td>\n",
       "      <td>0.812619</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.819524</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811571</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>15</td>\n",
       "      <td>0.882738</td>\n",
       "      <td>0.876786</td>\n",
       "      <td>0.877679</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.880631</td>\n",
       "      <td>0.003070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.741400</td>\n",
       "      <td>0.033856</td>\n",
       "      <td>0.016912</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>15</td>\n",
       "      <td>{'min_samples_leaf': 15}</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.813810</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.807381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814143</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>7</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.837976</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.839940</td>\n",
       "      <td>0.838274</td>\n",
       "      <td>0.001189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.661315</td>\n",
       "      <td>0.053633</td>\n",
       "      <td>0.024240</td>\n",
       "      <td>0.012289</td>\n",
       "      <td>25</td>\n",
       "      <td>{'min_samples_leaf': 25}</td>\n",
       "      <td>0.823095</td>\n",
       "      <td>0.818810</td>\n",
       "      <td>0.820952</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816143</td>\n",
       "      <td>0.007004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.828274</td>\n",
       "      <td>0.828631</td>\n",
       "      <td>0.827143</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.828095</td>\n",
       "      <td>0.828143</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.882173</td>\n",
       "      <td>0.137755</td>\n",
       "      <td>0.025032</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>35</td>\n",
       "      <td>{'min_samples_leaf': 35}</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.814048</td>\n",
       "      <td>0.819524</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813667</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>8</td>\n",
       "      <td>0.822976</td>\n",
       "      <td>0.823631</td>\n",
       "      <td>0.821845</td>\n",
       "      <td>0.824881</td>\n",
       "      <td>0.824940</td>\n",
       "      <td>0.823655</td>\n",
       "      <td>0.001174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.759617</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.022607</td>\n",
       "      <td>0.015081</td>\n",
       "      <td>45</td>\n",
       "      <td>{'min_samples_leaf': 45}</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814762</td>\n",
       "      <td>0.007656</td>\n",
       "      <td>3</td>\n",
       "      <td>0.822083</td>\n",
       "      <td>0.822976</td>\n",
       "      <td>0.819167</td>\n",
       "      <td>0.825060</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.821607</td>\n",
       "      <td>0.002372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.711624</td>\n",
       "      <td>0.042608</td>\n",
       "      <td>0.025454</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>55</td>\n",
       "      <td>{'min_samples_leaf': 55}</td>\n",
       "      <td>0.819762</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.815952</td>\n",
       "      <td>0.806905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814619</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>4</td>\n",
       "      <td>0.818571</td>\n",
       "      <td>0.817321</td>\n",
       "      <td>0.818571</td>\n",
       "      <td>0.823631</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.819619</td>\n",
       "      <td>0.002178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.636590</td>\n",
       "      <td>0.061422</td>\n",
       "      <td>0.017092</td>\n",
       "      <td>0.005879</td>\n",
       "      <td>65</td>\n",
       "      <td>{'min_samples_leaf': 65}</td>\n",
       "      <td>0.824048</td>\n",
       "      <td>0.821905</td>\n",
       "      <td>0.816905</td>\n",
       "      <td>0.804524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815571</td>\n",
       "      <td>0.007235</td>\n",
       "      <td>2</td>\n",
       "      <td>0.819464</td>\n",
       "      <td>0.818690</td>\n",
       "      <td>0.819702</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.817560</td>\n",
       "      <td>0.819369</td>\n",
       "      <td>0.001273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.671771</td>\n",
       "      <td>0.009747</td>\n",
       "      <td>0.015246</td>\n",
       "      <td>0.006180</td>\n",
       "      <td>75</td>\n",
       "      <td>{'min_samples_leaf': 75}</td>\n",
       "      <td>0.816190</td>\n",
       "      <td>0.810238</td>\n",
       "      <td>0.814762</td>\n",
       "      <td>0.804524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.004229</td>\n",
       "      <td>13</td>\n",
       "      <td>0.815952</td>\n",
       "      <td>0.811786</td>\n",
       "      <td>0.812917</td>\n",
       "      <td>0.822798</td>\n",
       "      <td>0.816369</td>\n",
       "      <td>0.815964</td>\n",
       "      <td>0.003837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.535954</td>\n",
       "      <td>0.040615</td>\n",
       "      <td>0.011929</td>\n",
       "      <td>0.002650</td>\n",
       "      <td>85</td>\n",
       "      <td>{'min_samples_leaf': 85}</td>\n",
       "      <td>0.822381</td>\n",
       "      <td>0.814048</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.801429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814381</td>\n",
       "      <td>0.007016</td>\n",
       "      <td>5</td>\n",
       "      <td>0.818155</td>\n",
       "      <td>0.814643</td>\n",
       "      <td>0.818036</td>\n",
       "      <td>0.821488</td>\n",
       "      <td>0.818929</td>\n",
       "      <td>0.818250</td>\n",
       "      <td>0.002191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.608047</td>\n",
       "      <td>0.113141</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.008319</td>\n",
       "      <td>95</td>\n",
       "      <td>{'min_samples_leaf': 95}</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.801905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812714</td>\n",
       "      <td>0.006002</td>\n",
       "      <td>12</td>\n",
       "      <td>0.812976</td>\n",
       "      <td>0.814643</td>\n",
       "      <td>0.818690</td>\n",
       "      <td>0.818095</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.816381</td>\n",
       "      <td>0.002197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.452835</td>\n",
       "      <td>0.064564</td>\n",
       "      <td>0.012683</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>105</td>\n",
       "      <td>{'min_samples_leaf': 105}</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.818095</td>\n",
       "      <td>0.813810</td>\n",
       "      <td>0.803810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814238</td>\n",
       "      <td>0.005986</td>\n",
       "      <td>6</td>\n",
       "      <td>0.818690</td>\n",
       "      <td>0.816548</td>\n",
       "      <td>0.808929</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.815179</td>\n",
       "      <td>0.815440</td>\n",
       "      <td>0.003467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.361668</td>\n",
       "      <td>0.023661</td>\n",
       "      <td>0.010599</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>115</td>\n",
       "      <td>{'min_samples_leaf': 115}</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.816429</td>\n",
       "      <td>0.800952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813571</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>9</td>\n",
       "      <td>0.815595</td>\n",
       "      <td>0.818214</td>\n",
       "      <td>0.814821</td>\n",
       "      <td>0.817738</td>\n",
       "      <td>0.815952</td>\n",
       "      <td>0.816464</td>\n",
       "      <td>0.001296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.381171</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>0.011731</td>\n",
       "      <td>0.005730</td>\n",
       "      <td>125</td>\n",
       "      <td>{'min_samples_leaf': 125}</td>\n",
       "      <td>0.818333</td>\n",
       "      <td>0.813810</td>\n",
       "      <td>0.815952</td>\n",
       "      <td>0.805238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813286</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>10</td>\n",
       "      <td>0.816250</td>\n",
       "      <td>0.815357</td>\n",
       "      <td>0.813393</td>\n",
       "      <td>0.820595</td>\n",
       "      <td>0.815893</td>\n",
       "      <td>0.816298</td>\n",
       "      <td>0.002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.398068</td>\n",
       "      <td>0.042935</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>135</td>\n",
       "      <td>{'min_samples_leaf': 135}</td>\n",
       "      <td>0.814524</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.799286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810619</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>17</td>\n",
       "      <td>0.809107</td>\n",
       "      <td>0.811250</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.816786</td>\n",
       "      <td>0.813214</td>\n",
       "      <td>0.813405</td>\n",
       "      <td>0.003007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.385192</td>\n",
       "      <td>0.044013</td>\n",
       "      <td>0.009815</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>145</td>\n",
       "      <td>{'min_samples_leaf': 145}</td>\n",
       "      <td>0.815952</td>\n",
       "      <td>0.812857</td>\n",
       "      <td>0.815952</td>\n",
       "      <td>0.797381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.007540</td>\n",
       "      <td>24</td>\n",
       "      <td>0.813571</td>\n",
       "      <td>0.808512</td>\n",
       "      <td>0.814881</td>\n",
       "      <td>0.811131</td>\n",
       "      <td>0.809405</td>\n",
       "      <td>0.811500</td>\n",
       "      <td>0.002416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.367541</td>\n",
       "      <td>0.016973</td>\n",
       "      <td>0.012464</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>155</td>\n",
       "      <td>{'min_samples_leaf': 155}</td>\n",
       "      <td>0.816429</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.813095</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810286</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>20</td>\n",
       "      <td>0.811905</td>\n",
       "      <td>0.806786</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.818452</td>\n",
       "      <td>0.809286</td>\n",
       "      <td>0.811286</td>\n",
       "      <td>0.003941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.347614</td>\n",
       "      <td>0.012803</td>\n",
       "      <td>0.010916</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>165</td>\n",
       "      <td>{'min_samples_leaf': 165}</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.818095</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.796429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812810</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>11</td>\n",
       "      <td>0.814940</td>\n",
       "      <td>0.813929</td>\n",
       "      <td>0.815655</td>\n",
       "      <td>0.812798</td>\n",
       "      <td>0.814048</td>\n",
       "      <td>0.814274</td>\n",
       "      <td>0.000970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.364762</td>\n",
       "      <td>0.019112</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>175</td>\n",
       "      <td>{'min_samples_leaf': 175}</td>\n",
       "      <td>0.815952</td>\n",
       "      <td>0.812857</td>\n",
       "      <td>0.814524</td>\n",
       "      <td>0.797857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809381</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>23</td>\n",
       "      <td>0.808095</td>\n",
       "      <td>0.813631</td>\n",
       "      <td>0.812083</td>\n",
       "      <td>0.814226</td>\n",
       "      <td>0.807024</td>\n",
       "      <td>0.811012</td>\n",
       "      <td>0.002924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.334718</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.008553</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>185</td>\n",
       "      <td>{'min_samples_leaf': 185}</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.815952</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811619</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>14</td>\n",
       "      <td>0.815595</td>\n",
       "      <td>0.810060</td>\n",
       "      <td>0.816071</td>\n",
       "      <td>0.819226</td>\n",
       "      <td>0.809048</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.003852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.320743</td>\n",
       "      <td>0.006742</td>\n",
       "      <td>0.008729</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>195</td>\n",
       "      <td>{'min_samples_leaf': 195}</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.810238</td>\n",
       "      <td>0.812857</td>\n",
       "      <td>0.802381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810571</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>18</td>\n",
       "      <td>0.808750</td>\n",
       "      <td>0.809167</td>\n",
       "      <td>0.810060</td>\n",
       "      <td>0.820774</td>\n",
       "      <td>0.811190</td>\n",
       "      <td>0.811988</td>\n",
       "      <td>0.004472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.347484</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>205</td>\n",
       "      <td>{'min_samples_leaf': 205}</td>\n",
       "      <td>0.818810</td>\n",
       "      <td>0.808810</td>\n",
       "      <td>0.816429</td>\n",
       "      <td>0.800714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>19</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.808036</td>\n",
       "      <td>0.816488</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.810595</td>\n",
       "      <td>0.813167</td>\n",
       "      <td>0.003515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.332574</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.011653</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>215</td>\n",
       "      <td>{'min_samples_leaf': 215}</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.812619</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.794762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809476</td>\n",
       "      <td>0.009301</td>\n",
       "      <td>21</td>\n",
       "      <td>0.813631</td>\n",
       "      <td>0.808571</td>\n",
       "      <td>0.807262</td>\n",
       "      <td>0.809821</td>\n",
       "      <td>0.807679</td>\n",
       "      <td>0.809393</td>\n",
       "      <td>0.002293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.406023</td>\n",
       "      <td>0.071644</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>225</td>\n",
       "      <td>{'min_samples_leaf': 225}</td>\n",
       "      <td>0.816190</td>\n",
       "      <td>0.819762</td>\n",
       "      <td>0.813095</td>\n",
       "      <td>0.798571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811190</td>\n",
       "      <td>0.007342</td>\n",
       "      <td>16</td>\n",
       "      <td>0.808095</td>\n",
       "      <td>0.813036</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>0.814405</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.810155</td>\n",
       "      <td>0.003185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.390398</td>\n",
       "      <td>0.046240</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.007541</td>\n",
       "      <td>235</td>\n",
       "      <td>{'min_samples_leaf': 235}</td>\n",
       "      <td>0.818095</td>\n",
       "      <td>0.807857</td>\n",
       "      <td>0.820714</td>\n",
       "      <td>0.796429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809429</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>22</td>\n",
       "      <td>0.810655</td>\n",
       "      <td>0.808631</td>\n",
       "      <td>0.817024</td>\n",
       "      <td>0.813571</td>\n",
       "      <td>0.810060</td>\n",
       "      <td>0.811988</td>\n",
       "      <td>0.002988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.348432</td>\n",
       "      <td>0.035419</td>\n",
       "      <td>0.012553</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>245</td>\n",
       "      <td>{'min_samples_leaf': 245}</td>\n",
       "      <td>0.814524</td>\n",
       "      <td>0.811667</td>\n",
       "      <td>0.818810</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807905</td>\n",
       "      <td>0.009361</td>\n",
       "      <td>28</td>\n",
       "      <td>0.806786</td>\n",
       "      <td>0.813512</td>\n",
       "      <td>0.816607</td>\n",
       "      <td>0.812083</td>\n",
       "      <td>0.806369</td>\n",
       "      <td>0.811071</td>\n",
       "      <td>0.003952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.325412</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.011167</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>255</td>\n",
       "      <td>{'min_samples_leaf': 255}</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.811190</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.799762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808381</td>\n",
       "      <td>0.006147</td>\n",
       "      <td>26</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.808393</td>\n",
       "      <td>0.804583</td>\n",
       "      <td>0.818512</td>\n",
       "      <td>0.811012</td>\n",
       "      <td>0.811357</td>\n",
       "      <td>0.004787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.314423</td>\n",
       "      <td>0.010647</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>265</td>\n",
       "      <td>{'min_samples_leaf': 265}</td>\n",
       "      <td>0.820714</td>\n",
       "      <td>0.809762</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.795952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807810</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>29</td>\n",
       "      <td>0.816071</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.808988</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.803452</td>\n",
       "      <td>0.808893</td>\n",
       "      <td>0.004186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.306800</td>\n",
       "      <td>0.013908</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>275</td>\n",
       "      <td>{'min_samples_leaf': 275}</td>\n",
       "      <td>0.819524</td>\n",
       "      <td>0.809048</td>\n",
       "      <td>0.810952</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808476</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>25</td>\n",
       "      <td>0.814702</td>\n",
       "      <td>0.808274</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>0.820060</td>\n",
       "      <td>0.805119</td>\n",
       "      <td>0.810821</td>\n",
       "      <td>0.005712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.309906</td>\n",
       "      <td>0.015246</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>285</td>\n",
       "      <td>{'min_samples_leaf': 285}</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.807857</td>\n",
       "      <td>0.805238</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>0.007947</td>\n",
       "      <td>36</td>\n",
       "      <td>0.810536</td>\n",
       "      <td>0.807440</td>\n",
       "      <td>0.800714</td>\n",
       "      <td>0.813512</td>\n",
       "      <td>0.806786</td>\n",
       "      <td>0.807798</td>\n",
       "      <td>0.004277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.298369</td>\n",
       "      <td>0.013752</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>295</td>\n",
       "      <td>{'min_samples_leaf': 295}</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.809286</td>\n",
       "      <td>0.808095</td>\n",
       "      <td>0.798333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807952</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>27</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.805774</td>\n",
       "      <td>0.802202</td>\n",
       "      <td>0.815893</td>\n",
       "      <td>0.809226</td>\n",
       "      <td>0.809286</td>\n",
       "      <td>0.004951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.314384</td>\n",
       "      <td>0.006065</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>305</td>\n",
       "      <td>{'min_samples_leaf': 305}</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.812619</td>\n",
       "      <td>0.794048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805238</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>38</td>\n",
       "      <td>0.799643</td>\n",
       "      <td>0.808393</td>\n",
       "      <td>0.809643</td>\n",
       "      <td>0.809702</td>\n",
       "      <td>0.809881</td>\n",
       "      <td>0.807452</td>\n",
       "      <td>0.003940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.313276</td>\n",
       "      <td>0.032230</td>\n",
       "      <td>0.012461</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>315</td>\n",
       "      <td>{'min_samples_leaf': 315}</td>\n",
       "      <td>0.812619</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810238</td>\n",
       "      <td>0.797143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804619</td>\n",
       "      <td>0.008121</td>\n",
       "      <td>39</td>\n",
       "      <td>0.803036</td>\n",
       "      <td>0.811071</td>\n",
       "      <td>0.807321</td>\n",
       "      <td>0.811905</td>\n",
       "      <td>0.801786</td>\n",
       "      <td>0.807024</td>\n",
       "      <td>0.004090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.343267</td>\n",
       "      <td>0.021111</td>\n",
       "      <td>0.010997</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>325</td>\n",
       "      <td>{'min_samples_leaf': 325}</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803952</td>\n",
       "      <td>0.007626</td>\n",
       "      <td>40</td>\n",
       "      <td>0.801190</td>\n",
       "      <td>0.805417</td>\n",
       "      <td>0.806131</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.803929</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.002376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.312347</td>\n",
       "      <td>0.013586</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>335</td>\n",
       "      <td>{'min_samples_leaf': 335}</td>\n",
       "      <td>0.810714</td>\n",
       "      <td>0.804048</td>\n",
       "      <td>0.804286</td>\n",
       "      <td>0.793810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801667</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>47</td>\n",
       "      <td>0.802143</td>\n",
       "      <td>0.800952</td>\n",
       "      <td>0.793929</td>\n",
       "      <td>0.805536</td>\n",
       "      <td>0.803036</td>\n",
       "      <td>0.801119</td>\n",
       "      <td>0.003898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.315363</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.011066</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>345</td>\n",
       "      <td>{'min_samples_leaf': 345}</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.810952</td>\n",
       "      <td>0.798095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805571</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>37</td>\n",
       "      <td>0.802440</td>\n",
       "      <td>0.803512</td>\n",
       "      <td>0.805893</td>\n",
       "      <td>0.816726</td>\n",
       "      <td>0.808750</td>\n",
       "      <td>0.807464</td>\n",
       "      <td>0.005114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.318289</td>\n",
       "      <td>0.014519</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>355</td>\n",
       "      <td>{'min_samples_leaf': 355}</td>\n",
       "      <td>0.816905</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.811667</td>\n",
       "      <td>0.797143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807143</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>30</td>\n",
       "      <td>0.806786</td>\n",
       "      <td>0.804226</td>\n",
       "      <td>0.806310</td>\n",
       "      <td>0.812024</td>\n",
       "      <td>0.806905</td>\n",
       "      <td>0.807250</td>\n",
       "      <td>0.002575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.319464</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>0.012767</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>365</td>\n",
       "      <td>{'min_samples_leaf': 365}</td>\n",
       "      <td>0.802619</td>\n",
       "      <td>0.808571</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.801190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806095</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>34</td>\n",
       "      <td>0.795238</td>\n",
       "      <td>0.809048</td>\n",
       "      <td>0.808690</td>\n",
       "      <td>0.815952</td>\n",
       "      <td>0.807679</td>\n",
       "      <td>0.807321</td>\n",
       "      <td>0.006715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.293350</td>\n",
       "      <td>0.026572</td>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>375</td>\n",
       "      <td>{'min_samples_leaf': 375}</td>\n",
       "      <td>0.811905</td>\n",
       "      <td>0.804286</td>\n",
       "      <td>0.808810</td>\n",
       "      <td>0.789286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802143</td>\n",
       "      <td>0.008274</td>\n",
       "      <td>45</td>\n",
       "      <td>0.805655</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.802321</td>\n",
       "      <td>0.803631</td>\n",
       "      <td>0.803810</td>\n",
       "      <td>0.803798</td>\n",
       "      <td>0.001068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.289934</td>\n",
       "      <td>0.024388</td>\n",
       "      <td>0.009415</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>385</td>\n",
       "      <td>{'min_samples_leaf': 385}</td>\n",
       "      <td>0.807857</td>\n",
       "      <td>0.802619</td>\n",
       "      <td>0.804286</td>\n",
       "      <td>0.797381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801333</td>\n",
       "      <td>0.004794</td>\n",
       "      <td>48</td>\n",
       "      <td>0.801131</td>\n",
       "      <td>0.803929</td>\n",
       "      <td>0.801786</td>\n",
       "      <td>0.813571</td>\n",
       "      <td>0.800893</td>\n",
       "      <td>0.804262</td>\n",
       "      <td>0.004776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.282832</td>\n",
       "      <td>0.023181</td>\n",
       "      <td>0.011359</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>395</td>\n",
       "      <td>{'min_samples_leaf': 395}</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.810714</td>\n",
       "      <td>0.793810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.007927</td>\n",
       "      <td>35</td>\n",
       "      <td>0.811548</td>\n",
       "      <td>0.806012</td>\n",
       "      <td>0.803810</td>\n",
       "      <td>0.807143</td>\n",
       "      <td>0.807798</td>\n",
       "      <td>0.807262</td>\n",
       "      <td>0.002536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.287687</td>\n",
       "      <td>0.025529</td>\n",
       "      <td>0.009815</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>405</td>\n",
       "      <td>{'min_samples_leaf': 405}</td>\n",
       "      <td>0.811190</td>\n",
       "      <td>0.804286</td>\n",
       "      <td>0.810238</td>\n",
       "      <td>0.794762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803857</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>41</td>\n",
       "      <td>0.802798</td>\n",
       "      <td>0.800595</td>\n",
       "      <td>0.804048</td>\n",
       "      <td>0.810119</td>\n",
       "      <td>0.804405</td>\n",
       "      <td>0.804393</td>\n",
       "      <td>0.003158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.288191</td>\n",
       "      <td>0.024370</td>\n",
       "      <td>0.010807</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>415</td>\n",
       "      <td>{'min_samples_leaf': 415}</td>\n",
       "      <td>0.809762</td>\n",
       "      <td>0.805238</td>\n",
       "      <td>0.810238</td>\n",
       "      <td>0.788571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802905</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>43</td>\n",
       "      <td>0.801310</td>\n",
       "      <td>0.802143</td>\n",
       "      <td>0.802024</td>\n",
       "      <td>0.801905</td>\n",
       "      <td>0.807440</td>\n",
       "      <td>0.802964</td>\n",
       "      <td>0.002256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.273766</td>\n",
       "      <td>0.011536</td>\n",
       "      <td>0.010255</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>425</td>\n",
       "      <td>{'min_samples_leaf': 425}</td>\n",
       "      <td>0.809048</td>\n",
       "      <td>0.799762</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.792381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802048</td>\n",
       "      <td>0.007430</td>\n",
       "      <td>46</td>\n",
       "      <td>0.800060</td>\n",
       "      <td>0.796190</td>\n",
       "      <td>0.808155</td>\n",
       "      <td>0.807917</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.802881</td>\n",
       "      <td>0.004616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.264686</td>\n",
       "      <td>0.010367</td>\n",
       "      <td>0.010172</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>435</td>\n",
       "      <td>{'min_samples_leaf': 435}</td>\n",
       "      <td>0.808571</td>\n",
       "      <td>0.809762</td>\n",
       "      <td>0.802381</td>\n",
       "      <td>0.787857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800238</td>\n",
       "      <td>0.008673</td>\n",
       "      <td>49</td>\n",
       "      <td>0.801726</td>\n",
       "      <td>0.805774</td>\n",
       "      <td>0.798810</td>\n",
       "      <td>0.806905</td>\n",
       "      <td>0.800655</td>\n",
       "      <td>0.802774</td>\n",
       "      <td>0.003078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.012618</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>445</td>\n",
       "      <td>{'min_samples_leaf': 445}</td>\n",
       "      <td>0.814048</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>0.814762</td>\n",
       "      <td>0.790952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807095</td>\n",
       "      <td>0.008673</td>\n",
       "      <td>31</td>\n",
       "      <td>0.803214</td>\n",
       "      <td>0.802619</td>\n",
       "      <td>0.804643</td>\n",
       "      <td>0.808750</td>\n",
       "      <td>0.811726</td>\n",
       "      <td>0.806190</td>\n",
       "      <td>0.003499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.276074</td>\n",
       "      <td>0.012516</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>455</td>\n",
       "      <td>{'min_samples_leaf': 455}</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>0.799048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802238</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>44</td>\n",
       "      <td>0.800595</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>0.801429</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>0.804619</td>\n",
       "      <td>0.004308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.280132</td>\n",
       "      <td>0.007970</td>\n",
       "      <td>0.008557</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>465</td>\n",
       "      <td>{'min_samples_leaf': 465}</td>\n",
       "      <td>0.811905</td>\n",
       "      <td>0.805238</td>\n",
       "      <td>0.810238</td>\n",
       "      <td>0.791429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803476</td>\n",
       "      <td>0.007604</td>\n",
       "      <td>42</td>\n",
       "      <td>0.802440</td>\n",
       "      <td>0.798452</td>\n",
       "      <td>0.804643</td>\n",
       "      <td>0.806607</td>\n",
       "      <td>0.802917</td>\n",
       "      <td>0.803012</td>\n",
       "      <td>0.002710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.246201</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.009566</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>475</td>\n",
       "      <td>{'min_samples_leaf': 475}</td>\n",
       "      <td>0.814762</td>\n",
       "      <td>0.814524</td>\n",
       "      <td>0.812857</td>\n",
       "      <td>0.788095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806714</td>\n",
       "      <td>0.010214</td>\n",
       "      <td>33</td>\n",
       "      <td>0.806548</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.800833</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>0.805583</td>\n",
       "      <td>0.002515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.294156</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>485</td>\n",
       "      <td>{'min_samples_leaf': 485}</td>\n",
       "      <td>0.816429</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.806905</td>\n",
       "      <td>0.794286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806952</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>32</td>\n",
       "      <td>0.805774</td>\n",
       "      <td>0.809821</td>\n",
       "      <td>0.802619</td>\n",
       "      <td>0.809107</td>\n",
       "      <td>0.808750</td>\n",
       "      <td>0.807214</td>\n",
       "      <td>0.002681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.249315</td>\n",
       "      <td>0.034712</td>\n",
       "      <td>0.007580</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>495</td>\n",
       "      <td>{'min_samples_leaf': 495}</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.807619</td>\n",
       "      <td>0.804762</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799048</td>\n",
       "      <td>0.010346</td>\n",
       "      <td>50</td>\n",
       "      <td>0.798512</td>\n",
       "      <td>0.804702</td>\n",
       "      <td>0.795595</td>\n",
       "      <td>0.794464</td>\n",
       "      <td>0.800893</td>\n",
       "      <td>0.798833</td>\n",
       "      <td>0.003698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.850318      0.030533         0.029697        0.012241   \n",
       "1        0.741400      0.033856         0.016912        0.002187   \n",
       "2        0.661315      0.053633         0.024240        0.012289   \n",
       "3        0.882173      0.137755         0.025032        0.008145   \n",
       "4        0.759617      0.011364         0.022607        0.015081   \n",
       "5        0.711624      0.042608         0.025454        0.020554   \n",
       "6        0.636590      0.061422         0.017092        0.005879   \n",
       "7        0.671771      0.009747         0.015246        0.006180   \n",
       "8        0.535954      0.040615         0.011929        0.002650   \n",
       "9        0.608047      0.113141         0.016386        0.008319   \n",
       "10       0.452835      0.064564         0.012683        0.004872   \n",
       "11       0.361668      0.023661         0.010599        0.003319   \n",
       "12       0.381171      0.009541         0.011731        0.005730   \n",
       "13       0.398068      0.042935         0.011225        0.001960   \n",
       "14       0.385192      0.044013         0.009815        0.001416   \n",
       "15       0.367541      0.016973         0.012464        0.001792   \n",
       "16       0.347614      0.012803         0.010916        0.002209   \n",
       "17       0.364762      0.019112         0.010660        0.003187   \n",
       "18       0.334718      0.018332         0.008553        0.001345   \n",
       "19       0.320743      0.006742         0.008729        0.000970   \n",
       "20       0.347484      0.014086         0.011884        0.003320   \n",
       "21       0.332574      0.005718         0.011653        0.003867   \n",
       "22       0.406023      0.071644         0.012000        0.004215   \n",
       "23       0.390398      0.046240         0.018370        0.007541   \n",
       "24       0.348432      0.035419         0.012553        0.003105   \n",
       "25       0.325412      0.015903         0.011167        0.002764   \n",
       "26       0.314423      0.010647         0.009641        0.002254   \n",
       "27       0.306800      0.013908         0.010000        0.002022   \n",
       "28       0.309906      0.015246         0.009063        0.001913   \n",
       "29       0.298369      0.013752         0.008418        0.001833   \n",
       "30       0.314384      0.006065         0.009650        0.002203   \n",
       "31       0.313276      0.032230         0.012461        0.002820   \n",
       "32       0.343267      0.021111         0.010997        0.001784   \n",
       "33       0.312347      0.013586         0.008735        0.001355   \n",
       "34       0.315363      0.019920         0.011066        0.001919   \n",
       "35       0.318289      0.014519         0.013449        0.003540   \n",
       "36       0.319464      0.011378         0.012767        0.003524   \n",
       "37       0.293350      0.026572         0.010413        0.003411   \n",
       "38       0.289934      0.024388         0.009415        0.002168   \n",
       "39       0.282832      0.023181         0.011359        0.003444   \n",
       "40       0.287687      0.025529         0.009815        0.002465   \n",
       "41       0.288191      0.024370         0.010807        0.005022   \n",
       "42       0.273766      0.011536         0.010255        0.003511   \n",
       "43       0.264686      0.010367         0.010172        0.002638   \n",
       "44       0.267400      0.012618         0.009299        0.004044   \n",
       "45       0.276074      0.012516         0.007705        0.000626   \n",
       "46       0.280132      0.007970         0.008557        0.002144   \n",
       "47       0.246201      0.015876         0.009566        0.000666   \n",
       "48       0.294156      0.006470         0.012497        0.003296   \n",
       "49       0.249315      0.034712         0.007580        0.001246   \n",
       "\n",
       "   param_min_samples_leaf                     params  split0_test_score  \\\n",
       "0                       5    {'min_samples_leaf': 5}           0.812619   \n",
       "1                      15   {'min_samples_leaf': 15}           0.822143   \n",
       "2                      25   {'min_samples_leaf': 25}           0.823095   \n",
       "3                      35   {'min_samples_leaf': 35}           0.817381   \n",
       "4                      45   {'min_samples_leaf': 45}           0.821667   \n",
       "5                      55   {'min_samples_leaf': 55}           0.819762   \n",
       "6                      65   {'min_samples_leaf': 65}           0.824048   \n",
       "7                      75   {'min_samples_leaf': 75}           0.816190   \n",
       "8                      85   {'min_samples_leaf': 85}           0.822381   \n",
       "9                      95   {'min_samples_leaf': 95}           0.817143   \n",
       "10                    105  {'min_samples_leaf': 105}           0.821667   \n",
       "11                    115  {'min_samples_leaf': 115}           0.816667   \n",
       "12                    125  {'min_samples_leaf': 125}           0.818333   \n",
       "13                    135  {'min_samples_leaf': 135}           0.814524   \n",
       "14                    145  {'min_samples_leaf': 145}           0.815952   \n",
       "15                    155  {'min_samples_leaf': 155}           0.816429   \n",
       "16                    165  {'min_samples_leaf': 165}           0.819048   \n",
       "17                    175  {'min_samples_leaf': 175}           0.815952   \n",
       "18                    185  {'min_samples_leaf': 185}           0.819286   \n",
       "19                    195  {'min_samples_leaf': 195}           0.819048   \n",
       "20                    205  {'min_samples_leaf': 205}           0.818810   \n",
       "21                    215  {'min_samples_leaf': 215}           0.822857   \n",
       "22                    225  {'min_samples_leaf': 225}           0.816190   \n",
       "23                    235  {'min_samples_leaf': 235}           0.818095   \n",
       "24                    245  {'min_samples_leaf': 245}           0.814524   \n",
       "25                    255  {'min_samples_leaf': 255}           0.817381   \n",
       "26                    265  {'min_samples_leaf': 265}           0.820714   \n",
       "27                    275  {'min_samples_leaf': 275}           0.819524   \n",
       "28                    285  {'min_samples_leaf': 285}           0.819048   \n",
       "29                    295  {'min_samples_leaf': 295}           0.819286   \n",
       "30                    305  {'min_samples_leaf': 305}           0.806667   \n",
       "31                    315  {'min_samples_leaf': 315}           0.812619   \n",
       "32                    325  {'min_samples_leaf': 325}           0.810476   \n",
       "33                    335  {'min_samples_leaf': 335}           0.810714   \n",
       "34                    345  {'min_samples_leaf': 345}           0.810000   \n",
       "35                    355  {'min_samples_leaf': 355}           0.816905   \n",
       "36                    365  {'min_samples_leaf': 365}           0.802619   \n",
       "37                    375  {'min_samples_leaf': 375}           0.811905   \n",
       "38                    385  {'min_samples_leaf': 385}           0.807857   \n",
       "39                    395  {'min_samples_leaf': 395}           0.817143   \n",
       "40                    405  {'min_samples_leaf': 405}           0.811190   \n",
       "41                    415  {'min_samples_leaf': 415}           0.809762   \n",
       "42                    425  {'min_samples_leaf': 425}           0.809048   \n",
       "43                    435  {'min_samples_leaf': 435}           0.808571   \n",
       "44                    445  {'min_samples_leaf': 445}           0.814048   \n",
       "45                    455  {'min_samples_leaf': 455}           0.806667   \n",
       "46                    465  {'min_samples_leaf': 465}           0.811905   \n",
       "47                    475  {'min_samples_leaf': 475}           0.814762   \n",
       "48                    485  {'min_samples_leaf': 485}           0.816429   \n",
       "49                    495  {'min_samples_leaf': 495}           0.806667   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0            0.814286           0.819524           0.802857  ...   \n",
       "1            0.813810           0.817381           0.807381  ...   \n",
       "2            0.818810           0.820952           0.803333  ...   \n",
       "3            0.814048           0.819524           0.803571  ...   \n",
       "4            0.822143           0.819048           0.805714  ...   \n",
       "5            0.817381           0.815952           0.806905  ...   \n",
       "6            0.821905           0.816905           0.804524  ...   \n",
       "7            0.810238           0.814762           0.804524  ...   \n",
       "8            0.814048           0.817381           0.801429  ...   \n",
       "9            0.812143           0.819286           0.801905  ...   \n",
       "10           0.818095           0.813810           0.803810  ...   \n",
       "11           0.821667           0.816429           0.800952  ...   \n",
       "12           0.813810           0.815952           0.805238  ...   \n",
       "13           0.811429           0.817857           0.799286  ...   \n",
       "14           0.812857           0.815952           0.797381  ...   \n",
       "15           0.811429           0.813095           0.805000  ...   \n",
       "16           0.818095           0.819286           0.796429  ...   \n",
       "17           0.812857           0.814524           0.797857  ...   \n",
       "18           0.810000           0.815952           0.806429  ...   \n",
       "19           0.810238           0.812857           0.802381  ...   \n",
       "20           0.808810           0.816429           0.800714  ...   \n",
       "21           0.812619           0.812143           0.794762  ...   \n",
       "22           0.819762           0.813095           0.798571  ...   \n",
       "23           0.807857           0.820714           0.796429  ...   \n",
       "24           0.811667           0.818810           0.793095  ...   \n",
       "25           0.811190           0.810000           0.799762  ...   \n",
       "26           0.809762           0.813333           0.795952  ...   \n",
       "27           0.809048           0.810952           0.803571  ...   \n",
       "28           0.807857           0.805238           0.795000  ...   \n",
       "29           0.809286           0.808095           0.798333  ...   \n",
       "30           0.808333           0.812619           0.794048  ...   \n",
       "31           0.810476           0.810238           0.797143  ...   \n",
       "32           0.806429           0.812143           0.791667  ...   \n",
       "33           0.804048           0.804286           0.793810  ...   \n",
       "34           0.803571           0.810952           0.798095  ...   \n",
       "35           0.808333           0.811667           0.797143  ...   \n",
       "36           0.808571           0.812143           0.801190  ...   \n",
       "37           0.804286           0.808810           0.789286  ...   \n",
       "38           0.802619           0.804286           0.797381  ...   \n",
       "39           0.806667           0.810714           0.793810  ...   \n",
       "40           0.804286           0.810238           0.794762  ...   \n",
       "41           0.805238           0.810238           0.788571  ...   \n",
       "42           0.799762           0.812143           0.792381  ...   \n",
       "43           0.809762           0.802381           0.787857  ...   \n",
       "44           0.805952           0.814762           0.790952  ...   \n",
       "45           0.805952           0.805714           0.799048  ...   \n",
       "46           0.805238           0.810238           0.791429  ...   \n",
       "47           0.814524           0.812857           0.788095  ...   \n",
       "48           0.811429           0.806905           0.794286  ...   \n",
       "49           0.807619           0.804762           0.780000  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0          0.811571        0.005597               15            0.882738   \n",
       "1          0.814143        0.005244                7            0.837500   \n",
       "2          0.816143        0.007004                1            0.828274   \n",
       "3          0.813667        0.005481                8            0.822976   \n",
       "4          0.814762        0.007656                3            0.822083   \n",
       "5          0.814619        0.004420                4            0.818571   \n",
       "6          0.815571        0.007235                2            0.819464   \n",
       "7          0.812000        0.004229               13            0.815952   \n",
       "8          0.814381        0.007016                5            0.818155   \n",
       "9          0.812714        0.006002               12            0.812976   \n",
       "10         0.814238        0.005986                6            0.818690   \n",
       "11         0.813571        0.006994                9            0.815595   \n",
       "12         0.813286        0.004420               10            0.816250   \n",
       "13         0.810619        0.006279               17            0.809107   \n",
       "14         0.809000        0.007540               24            0.813571   \n",
       "15         0.810286        0.004427               20            0.811905   \n",
       "16         0.812810        0.008715               11            0.814940   \n",
       "17         0.809381        0.006752               23            0.808095   \n",
       "18         0.811619        0.005178               14            0.815595   \n",
       "19         0.810571        0.005464               18            0.808750   \n",
       "20         0.810476        0.006498               19            0.813333   \n",
       "21         0.809476        0.009301               21            0.813631   \n",
       "22         0.811190        0.007342               16            0.808095   \n",
       "23         0.809429        0.008977               22            0.810655   \n",
       "24         0.807905        0.009361               28            0.806786   \n",
       "25         0.808381        0.006147               26            0.814286   \n",
       "26         0.807810        0.009101               29            0.816071   \n",
       "27         0.808476        0.006885               25            0.814702   \n",
       "28         0.805714        0.007947               36            0.810536   \n",
       "29         0.807952        0.006826               27            0.813333   \n",
       "30         0.805238        0.006194               38            0.799643   \n",
       "31         0.804619        0.008121               39            0.803036   \n",
       "32         0.803952        0.007626               40            0.801190   \n",
       "33         0.801667        0.006236               47            0.802143   \n",
       "34         0.805571        0.004660               37            0.802440   \n",
       "35         0.807143        0.007031               30            0.806786   \n",
       "36         0.806095        0.003972               34            0.795238   \n",
       "37         0.802143        0.008274               45            0.805655   \n",
       "38         0.801333        0.004794               48            0.801131   \n",
       "39         0.806000        0.007927               35            0.811548   \n",
       "40         0.803857        0.006370               41            0.802798   \n",
       "41         0.802905        0.007956               43            0.801310   \n",
       "42         0.802048        0.007430               46            0.800060   \n",
       "43         0.800238        0.008673               49            0.801726   \n",
       "44         0.807095        0.008673               31            0.803214   \n",
       "45         0.802238        0.005034               44            0.800595   \n",
       "46         0.803476        0.007604               42            0.802440   \n",
       "47         0.806714        0.010214               33            0.806548   \n",
       "48         0.806952        0.007373               32            0.805774   \n",
       "49         0.799048        0.010346               50            0.798512   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0             0.876786            0.877679            0.885000   \n",
       "1             0.837976            0.836667            0.839286   \n",
       "2             0.828631            0.827143            0.828571   \n",
       "3             0.823631            0.821845            0.824881   \n",
       "4             0.822976            0.819167            0.825060   \n",
       "5             0.817321            0.818571            0.823631   \n",
       "6             0.818690            0.819702            0.821429   \n",
       "7             0.811786            0.812917            0.822798   \n",
       "8             0.814643            0.818036            0.821488   \n",
       "9             0.814643            0.818690            0.818095   \n",
       "10            0.816548            0.808929            0.817857   \n",
       "11            0.818214            0.814821            0.817738   \n",
       "12            0.815357            0.813393            0.820595   \n",
       "13            0.811250            0.816667            0.816786   \n",
       "14            0.808512            0.814881            0.811131   \n",
       "15            0.806786            0.810000            0.818452   \n",
       "16            0.813929            0.815655            0.812798   \n",
       "17            0.813631            0.812083            0.814226   \n",
       "18            0.810060            0.816071            0.819226   \n",
       "19            0.809167            0.810060            0.820774   \n",
       "20            0.808036            0.816488            0.817381   \n",
       "21            0.808571            0.807262            0.809821   \n",
       "22            0.813036            0.805714            0.814405   \n",
       "23            0.808631            0.817024            0.813571   \n",
       "24            0.813512            0.816607            0.812083   \n",
       "25            0.808393            0.804583            0.818512   \n",
       "26            0.806429            0.808988            0.809524   \n",
       "27            0.808274            0.805952            0.820060   \n",
       "28            0.807440            0.800714            0.813512   \n",
       "29            0.805774            0.802202            0.815893   \n",
       "30            0.808393            0.809643            0.809702   \n",
       "31            0.811071            0.807321            0.811905   \n",
       "32            0.805417            0.806131            0.808333   \n",
       "33            0.800952            0.793929            0.805536   \n",
       "34            0.803512            0.805893            0.816726   \n",
       "35            0.804226            0.806310            0.812024   \n",
       "36            0.809048            0.808690            0.815952   \n",
       "37            0.803571            0.802321            0.803631   \n",
       "38            0.803929            0.801786            0.813571   \n",
       "39            0.806012            0.803810            0.807143   \n",
       "40            0.800595            0.804048            0.810119   \n",
       "41            0.802143            0.802024            0.801905   \n",
       "42            0.796190            0.808155            0.807917   \n",
       "43            0.805774            0.798810            0.806905   \n",
       "44            0.802619            0.804643            0.808750   \n",
       "45            0.805714            0.801429            0.812500   \n",
       "46            0.798452            0.804643            0.806607   \n",
       "47            0.808333            0.806250            0.800833   \n",
       "48            0.809821            0.802619            0.809107   \n",
       "49            0.804702            0.795595            0.794464   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0             0.880952          0.880631         0.003070  \n",
       "1             0.839940          0.838274         0.001189  \n",
       "2             0.828095          0.828143         0.000537  \n",
       "3             0.824940          0.823655         0.001174  \n",
       "4             0.818750          0.821607         0.002372  \n",
       "5             0.820000          0.819619         0.002178  \n",
       "6             0.817560          0.819369         0.001273  \n",
       "7             0.816369          0.815964         0.003837  \n",
       "8             0.818929          0.818250         0.002191  \n",
       "9             0.817500          0.816381         0.002197  \n",
       "10            0.815179          0.815440         0.003467  \n",
       "11            0.815952          0.816464         0.001296  \n",
       "12            0.815893          0.816298         0.002365  \n",
       "13            0.813214          0.813405         0.003007  \n",
       "14            0.809405          0.811500         0.002416  \n",
       "15            0.809286          0.811286         0.003941  \n",
       "16            0.814048          0.814274         0.000970  \n",
       "17            0.807024          0.811012         0.002924  \n",
       "18            0.809048          0.814000         0.003852  \n",
       "19            0.811190          0.811988         0.004472  \n",
       "20            0.810595          0.813167         0.003515  \n",
       "21            0.807679          0.809393         0.002293  \n",
       "22            0.809524          0.810155         0.003185  \n",
       "23            0.810060          0.811988         0.002988  \n",
       "24            0.806369          0.811071         0.003952  \n",
       "25            0.811012          0.811357         0.004787  \n",
       "26            0.803452          0.808893         0.004186  \n",
       "27            0.805119          0.810821         0.005712  \n",
       "28            0.806786          0.807798         0.004277  \n",
       "29            0.809226          0.809286         0.004951  \n",
       "30            0.809881          0.807452         0.003940  \n",
       "31            0.801786          0.807024         0.004090  \n",
       "32            0.803929          0.805000         0.002376  \n",
       "33            0.803036          0.801119         0.003898  \n",
       "34            0.808750          0.807464         0.005114  \n",
       "35            0.806905          0.807250         0.002575  \n",
       "36            0.807679          0.807321         0.006715  \n",
       "37            0.803810          0.803798         0.001068  \n",
       "38            0.800893          0.804262         0.004776  \n",
       "39            0.807798          0.807262         0.002536  \n",
       "40            0.804405          0.804393         0.003158  \n",
       "41            0.807440          0.802964         0.002256  \n",
       "42            0.802083          0.802881         0.004616  \n",
       "43            0.800655          0.802774         0.003078  \n",
       "44            0.811726          0.806190         0.003499  \n",
       "45            0.802857          0.804619         0.004308  \n",
       "46            0.802917          0.803012         0.002710  \n",
       "47            0.805952          0.805583         0.002515  \n",
       "48            0.808750          0.807214         0.002681  \n",
       "49            0.800893          0.798833         0.003698  \n",
       "\n",
       "[50 rows x 21 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, random_state=True, shuffle=True)\n",
    "params = {'min_samples_leaf': range(5, 500, 10)}\n",
    "gs_model3 = GridSearchCV(rf_model,\n",
    "                         param_grid=params,\n",
    "                         cv=folds,\n",
    "                         verbose=1,\n",
    "                         n_jobs=-1,\n",
    "                         return_train_score=True,\n",
    "                         scoring='accuracy')\n",
    "gs_model3.fit(x_train, y_train)\n",
    "results = pd.DataFrame(gs_model3.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(results['param_min_samples_leaf'], results['mean_train_score'], label='Training Accuracy')\n",
    "plt.plot(results['param_min_samples_leaf'], results['mean_test_score'], label='Test Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 25}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model3.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_estimators: No.of Decision tree estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.758304</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10}</td>\n",
       "      <td>0.811190</td>\n",
       "      <td>0.812381</td>\n",
       "      <td>0.809286</td>\n",
       "      <td>0.796905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805381</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>10</td>\n",
       "      <td>0.983214</td>\n",
       "      <td>0.980238</td>\n",
       "      <td>0.981190</td>\n",
       "      <td>0.979881</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>0.981274</td>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.154241</td>\n",
       "      <td>0.052103</td>\n",
       "      <td>0.098786</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>60</td>\n",
       "      <td>{'n_estimators': 60}</td>\n",
       "      <td>0.816429</td>\n",
       "      <td>0.817619</td>\n",
       "      <td>0.818571</td>\n",
       "      <td>0.805238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813714</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.999821</td>\n",
       "      <td>0.999643</td>\n",
       "      <td>0.999821</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.028417</td>\n",
       "      <td>0.321965</td>\n",
       "      <td>0.166605</td>\n",
       "      <td>0.013612</td>\n",
       "      <td>110</td>\n",
       "      <td>{'n_estimators': 110}</td>\n",
       "      <td>0.816905</td>\n",
       "      <td>0.814524</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.807857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814048</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.079311</td>\n",
       "      <td>0.135134</td>\n",
       "      <td>0.283682</td>\n",
       "      <td>0.018875</td>\n",
       "      <td>160</td>\n",
       "      <td>{'n_estimators': 160}</td>\n",
       "      <td>0.820476</td>\n",
       "      <td>0.817619</td>\n",
       "      <td>0.819762</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815667</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.985192</td>\n",
       "      <td>0.134829</td>\n",
       "      <td>0.349729</td>\n",
       "      <td>0.031642</td>\n",
       "      <td>210</td>\n",
       "      <td>{'n_estimators': 210}</td>\n",
       "      <td>0.817619</td>\n",
       "      <td>0.816429</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.807857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814333</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.445204</td>\n",
       "      <td>1.077968</td>\n",
       "      <td>0.536043</td>\n",
       "      <td>0.189893</td>\n",
       "      <td>260</td>\n",
       "      <td>{'n_estimators': 260}</td>\n",
       "      <td>0.819762</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.820238</td>\n",
       "      <td>0.808810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814667</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32.896848</td>\n",
       "      <td>2.829920</td>\n",
       "      <td>0.887491</td>\n",
       "      <td>0.354063</td>\n",
       "      <td>310</td>\n",
       "      <td>{'n_estimators': 310}</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.815476</td>\n",
       "      <td>0.820476</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815238</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28.062704</td>\n",
       "      <td>2.014034</td>\n",
       "      <td>0.564701</td>\n",
       "      <td>0.048070</td>\n",
       "      <td>360</td>\n",
       "      <td>{'n_estimators': 360}</td>\n",
       "      <td>0.823095</td>\n",
       "      <td>0.814524</td>\n",
       "      <td>0.819762</td>\n",
       "      <td>0.808571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815190</td>\n",
       "      <td>0.005560</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28.291890</td>\n",
       "      <td>0.167734</td>\n",
       "      <td>0.613662</td>\n",
       "      <td>0.046277</td>\n",
       "      <td>410</td>\n",
       "      <td>{'n_estimators': 410}</td>\n",
       "      <td>0.818333</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.822619</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814190</td>\n",
       "      <td>0.005853</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29.032591</td>\n",
       "      <td>3.277920</td>\n",
       "      <td>0.551056</td>\n",
       "      <td>0.121769</td>\n",
       "      <td>460</td>\n",
       "      <td>{'n_estimators': 460}</td>\n",
       "      <td>0.818810</td>\n",
       "      <td>0.815476</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.807857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814429</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.758304      0.025961         0.017520        0.002359   \n",
       "1       4.154241      0.052103         0.098786        0.007423   \n",
       "2       8.028417      0.321965         0.166605        0.013612   \n",
       "3      12.079311      0.135134         0.283682        0.018875   \n",
       "4      15.985192      0.134829         0.349729        0.031642   \n",
       "5      20.445204      1.077968         0.536043        0.189893   \n",
       "6      32.896848      2.829920         0.887491        0.354063   \n",
       "7      28.062704      2.014034         0.564701        0.048070   \n",
       "8      28.291890      0.167734         0.613662        0.046277   \n",
       "9      29.032591      3.277920         0.551056        0.121769   \n",
       "\n",
       "  param_n_estimators                 params  split0_test_score  \\\n",
       "0                 10   {'n_estimators': 10}           0.811190   \n",
       "1                 60   {'n_estimators': 60}           0.816429   \n",
       "2                110  {'n_estimators': 110}           0.816905   \n",
       "3                160  {'n_estimators': 160}           0.820476   \n",
       "4                210  {'n_estimators': 210}           0.817619   \n",
       "5                260  {'n_estimators': 260}           0.819762   \n",
       "6                310  {'n_estimators': 310}           0.817381   \n",
       "7                360  {'n_estimators': 360}           0.823095   \n",
       "8                410  {'n_estimators': 410}           0.818333   \n",
       "9                460  {'n_estimators': 460}           0.818810   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           0.812381           0.809286           0.796905  ...   \n",
       "1           0.817619           0.818571           0.805238  ...   \n",
       "2           0.814524           0.821429           0.807857  ...   \n",
       "3           0.817619           0.819762           0.810476  ...   \n",
       "4           0.816429           0.821429           0.807857  ...   \n",
       "5           0.814286           0.820238           0.808810  ...   \n",
       "6           0.815476           0.820476           0.811429  ...   \n",
       "7           0.814524           0.819762           0.808571  ...   \n",
       "8           0.814286           0.822619           0.806667  ...   \n",
       "9           0.815476           0.821667           0.807857  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.805381        0.006895               10            0.983214   \n",
       "1         0.813714        0.005041                9            0.999762   \n",
       "2         0.814048        0.004933                8            0.999940   \n",
       "3         0.815667        0.004534                1            1.000000   \n",
       "4         0.814333        0.005357                6            1.000000   \n",
       "5         0.814667        0.004713                4            1.000000   \n",
       "6         0.815238        0.003496                2            1.000000   \n",
       "7         0.815190        0.005560                3            1.000000   \n",
       "8         0.814190        0.005853                7            1.000000   \n",
       "9         0.814429        0.005532                5            1.000000   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            0.980238            0.981190            0.979881   \n",
       "1            0.999762            0.999821            0.999643   \n",
       "2            1.000000            0.999940            1.000000   \n",
       "3            1.000000            1.000000            1.000000   \n",
       "4            1.000000            1.000000            1.000000   \n",
       "5            1.000000            1.000000            1.000000   \n",
       "6            1.000000            1.000000            1.000000   \n",
       "7            1.000000            1.000000            1.000000   \n",
       "8            1.000000            1.000000            1.000000   \n",
       "9            1.000000            1.000000            1.000000   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0            0.981845          0.981274         0.001193  \n",
       "1            0.999821          0.999762         0.000065  \n",
       "2            1.000000          0.999976         0.000029  \n",
       "3            1.000000          1.000000         0.000000  \n",
       "4            1.000000          1.000000         0.000000  \n",
       "5            1.000000          1.000000         0.000000  \n",
       "6            1.000000          1.000000         0.000000  \n",
       "7            1.000000          1.000000         0.000000  \n",
       "8            1.000000          1.000000         0.000000  \n",
       "9            1.000000          1.000000         0.000000  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, random_state=True, shuffle=True)\n",
    "params = {'n_estimators': range(10, 500, 50)}\n",
    "gs_model4 = GridSearchCV(rf_model,\n",
    "                         param_grid=params,\n",
    "                         verbose=1,\n",
    "                         n_jobs=-1,\n",
    "                         scoring='accuracy',\n",
    "                         return_train_score=True,\n",
    "                         cv=folds)\n",
    "gs_model4.fit(x_train, y_train)\n",
    "results = pd.DataFrame(gs_model4.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(results['param_n_estimators'], results['mean_train_score'], label='Training accuracy')\n",
    "plt.plot(results['param_n_estimators'], results['mean_test_score'], label='Test accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'LIMIT_BAL',\n",
       " 'SEX',\n",
       " 'EDUCATION',\n",
       " 'MARRIAGE',\n",
       " 'AGE',\n",
       " 'PAY_0',\n",
       " 'PAY_2',\n",
       " 'PAY_3',\n",
       " 'PAY_4',\n",
       " 'PAY_5',\n",
       " 'PAY_6',\n",
       " 'BILL_AMT1',\n",
       " 'BILL_AMT2',\n",
       " 'BILL_AMT3',\n",
       " 'BILL_AMT4',\n",
       " 'BILL_AMT5',\n",
       " 'BILL_AMT6',\n",
       " 'PAY_AMT1',\n",
       " 'PAY_AMT2',\n",
       " 'PAY_AMT3',\n",
       " 'PAY_AMT4',\n",
       " 'PAY_AMT5',\n",
       " 'PAY_AMT6']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   58.5s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.289858</td>\n",
       "      <td>0.018863</td>\n",
       "      <td>0.022023</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_features': 1}</td>\n",
       "      <td>0.801667</td>\n",
       "      <td>0.802619</td>\n",
       "      <td>0.806190</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798952</td>\n",
       "      <td>0.005857</td>\n",
       "      <td>14</td>\n",
       "      <td>0.980476</td>\n",
       "      <td>0.979702</td>\n",
       "      <td>0.981190</td>\n",
       "      <td>0.980833</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.980869</td>\n",
       "      <td>0.000805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.394891</td>\n",
       "      <td>0.013276</td>\n",
       "      <td>0.024182</td>\n",
       "      <td>0.009047</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_features': 2}</td>\n",
       "      <td>0.803810</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.800476</td>\n",
       "      <td>0.793810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799810</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>13</td>\n",
       "      <td>0.981548</td>\n",
       "      <td>0.979405</td>\n",
       "      <td>0.980119</td>\n",
       "      <td>0.979702</td>\n",
       "      <td>0.979524</td>\n",
       "      <td>0.980060</td>\n",
       "      <td>0.000782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.546582</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.017375</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_features': 3}</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>0.807381</td>\n",
       "      <td>0.807619</td>\n",
       "      <td>0.800952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802524</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>12</td>\n",
       "      <td>0.980238</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.978988</td>\n",
       "      <td>0.980417</td>\n",
       "      <td>0.980655</td>\n",
       "      <td>0.980488</td>\n",
       "      <td>0.001008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.699520</td>\n",
       "      <td>0.023151</td>\n",
       "      <td>0.021931</td>\n",
       "      <td>0.007625</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_features': 4}</td>\n",
       "      <td>0.809286</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.798333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804381</td>\n",
       "      <td>0.006171</td>\n",
       "      <td>5</td>\n",
       "      <td>0.980476</td>\n",
       "      <td>0.978988</td>\n",
       "      <td>0.979821</td>\n",
       "      <td>0.981905</td>\n",
       "      <td>0.982738</td>\n",
       "      <td>0.980786</td>\n",
       "      <td>0.001365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.810892</td>\n",
       "      <td>0.027783</td>\n",
       "      <td>0.018694</td>\n",
       "      <td>0.003651</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_features': 5}</td>\n",
       "      <td>0.812619</td>\n",
       "      <td>0.801667</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.797381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803238</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>9</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.979464</td>\n",
       "      <td>0.979524</td>\n",
       "      <td>0.981548</td>\n",
       "      <td>0.981369</td>\n",
       "      <td>0.980381</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.952415</td>\n",
       "      <td>0.032455</td>\n",
       "      <td>0.015944</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_features': 6}</td>\n",
       "      <td>0.809286</td>\n",
       "      <td>0.806905</td>\n",
       "      <td>0.808095</td>\n",
       "      <td>0.795952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804476</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>4</td>\n",
       "      <td>0.978810</td>\n",
       "      <td>0.979702</td>\n",
       "      <td>0.980893</td>\n",
       "      <td>0.982560</td>\n",
       "      <td>0.981607</td>\n",
       "      <td>0.980714</td>\n",
       "      <td>0.001333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.077383</td>\n",
       "      <td>0.042439</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_features': 7}</td>\n",
       "      <td>0.812619</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.811190</td>\n",
       "      <td>0.795476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805571</td>\n",
       "      <td>0.006123</td>\n",
       "      <td>2</td>\n",
       "      <td>0.980357</td>\n",
       "      <td>0.980536</td>\n",
       "      <td>0.980655</td>\n",
       "      <td>0.981190</td>\n",
       "      <td>0.980179</td>\n",
       "      <td>0.980583</td>\n",
       "      <td>0.000344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.239989</td>\n",
       "      <td>0.038130</td>\n",
       "      <td>0.016348</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>8</td>\n",
       "      <td>{'max_features': 8}</td>\n",
       "      <td>0.810952</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>0.807381</td>\n",
       "      <td>0.799286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803905</td>\n",
       "      <td>0.005401</td>\n",
       "      <td>7</td>\n",
       "      <td>0.981310</td>\n",
       "      <td>0.980238</td>\n",
       "      <td>0.979881</td>\n",
       "      <td>0.979940</td>\n",
       "      <td>0.979345</td>\n",
       "      <td>0.980143</td>\n",
       "      <td>0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.383680</td>\n",
       "      <td>0.024862</td>\n",
       "      <td>0.014935</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>9</td>\n",
       "      <td>{'max_features': 9}</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.798571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804190</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>6</td>\n",
       "      <td>0.980238</td>\n",
       "      <td>0.979345</td>\n",
       "      <td>0.979286</td>\n",
       "      <td>0.981786</td>\n",
       "      <td>0.978750</td>\n",
       "      <td>0.979881</td>\n",
       "      <td>0.001065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.638842</td>\n",
       "      <td>0.045852</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_features': 10}</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.809762</td>\n",
       "      <td>0.795714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803143</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>10</td>\n",
       "      <td>0.981131</td>\n",
       "      <td>0.980595</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.981131</td>\n",
       "      <td>0.979821</td>\n",
       "      <td>0.980536</td>\n",
       "      <td>0.000549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.752799</td>\n",
       "      <td>0.024907</td>\n",
       "      <td>0.017996</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>11</td>\n",
       "      <td>{'max_features': 11}</td>\n",
       "      <td>0.811190</td>\n",
       "      <td>0.809048</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.790952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805857</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980893</td>\n",
       "      <td>0.979048</td>\n",
       "      <td>0.979464</td>\n",
       "      <td>0.979881</td>\n",
       "      <td>0.981369</td>\n",
       "      <td>0.980131</td>\n",
       "      <td>0.000871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.910085</td>\n",
       "      <td>0.056632</td>\n",
       "      <td>0.019209</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>12</td>\n",
       "      <td>{'max_features': 12}</td>\n",
       "      <td>0.812381</td>\n",
       "      <td>0.802381</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804952</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>3</td>\n",
       "      <td>0.981012</td>\n",
       "      <td>0.979821</td>\n",
       "      <td>0.980595</td>\n",
       "      <td>0.981667</td>\n",
       "      <td>0.979464</td>\n",
       "      <td>0.980512</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.232286</td>\n",
       "      <td>0.065308</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>13</td>\n",
       "      <td>{'max_features': 13}</td>\n",
       "      <td>0.805238</td>\n",
       "      <td>0.806905</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>0.799524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802905</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>11</td>\n",
       "      <td>0.981190</td>\n",
       "      <td>0.978452</td>\n",
       "      <td>0.979821</td>\n",
       "      <td>0.980595</td>\n",
       "      <td>0.980595</td>\n",
       "      <td>0.980131</td>\n",
       "      <td>0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.084409</td>\n",
       "      <td>0.198334</td>\n",
       "      <td>0.013854</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>14</td>\n",
       "      <td>{'max_features': 14}</td>\n",
       "      <td>0.807619</td>\n",
       "      <td>0.806190</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.795714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803476</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>8</td>\n",
       "      <td>0.979702</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>0.978631</td>\n",
       "      <td>0.981607</td>\n",
       "      <td>0.980238</td>\n",
       "      <td>0.980286</td>\n",
       "      <td>0.001073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.289858      0.018863         0.022023        0.002231   \n",
       "1        0.394891      0.013276         0.024182        0.009047   \n",
       "2        0.546582      0.009912         0.017375        0.000865   \n",
       "3        0.699520      0.023151         0.021931        0.007625   \n",
       "4        0.810892      0.027783         0.018694        0.003651   \n",
       "5        0.952415      0.032455         0.015944        0.001018   \n",
       "6        1.077383      0.042439         0.018579        0.006100   \n",
       "7        1.239989      0.038130         0.016348        0.002116   \n",
       "8        1.383680      0.024862         0.014935        0.001299   \n",
       "9        1.638842      0.045852         0.018166        0.003088   \n",
       "10       1.752799      0.024907         0.017996        0.003766   \n",
       "11       1.910085      0.056632         0.019209        0.004237   \n",
       "12       2.232286      0.065308         0.016401        0.003468   \n",
       "13       2.084409      0.198334         0.013854        0.002293   \n",
       "\n",
       "   param_max_features                params  split0_test_score  \\\n",
       "0                   1   {'max_features': 1}           0.801667   \n",
       "1                   2   {'max_features': 2}           0.803810   \n",
       "2                   3   {'max_features': 3}           0.802857   \n",
       "3                   4   {'max_features': 4}           0.809286   \n",
       "4                   5   {'max_features': 5}           0.812619   \n",
       "5                   6   {'max_features': 6}           0.809286   \n",
       "6                   7   {'max_features': 7}           0.812619   \n",
       "7                   8   {'max_features': 8}           0.810952   \n",
       "8                   9   {'max_features': 9}           0.810476   \n",
       "9                  10  {'max_features': 10}           0.810476   \n",
       "10                 11  {'max_features': 11}           0.811190   \n",
       "11                 12  {'max_features': 12}           0.812381   \n",
       "12                 13  {'max_features': 13}           0.805238   \n",
       "13                 14  {'max_features': 14}           0.807619   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0            0.802619           0.806190           0.790476  ...   \n",
       "1            0.803333           0.800476           0.793810  ...   \n",
       "2            0.807381           0.807619           0.800952  ...   \n",
       "3            0.805952           0.812143           0.798333  ...   \n",
       "4            0.801667           0.806429           0.797381  ...   \n",
       "5            0.806905           0.808095           0.795952  ...   \n",
       "6            0.805000           0.811190           0.795476  ...   \n",
       "7            0.805714           0.807381           0.799286  ...   \n",
       "8            0.805952           0.805000           0.798571  ...   \n",
       "9            0.806429           0.809762           0.795714  ...   \n",
       "10           0.809048           0.810476           0.790952  ...   \n",
       "11           0.802381           0.805714           0.803571  ...   \n",
       "12           0.806905           0.805952           0.799524  ...   \n",
       "13           0.806190           0.806429           0.795714  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0          0.798952        0.005857               14            0.980476   \n",
       "1          0.799810        0.003733               13            0.981548   \n",
       "2          0.802524        0.005061               12            0.980238   \n",
       "3          0.804381        0.006171                5            0.980476   \n",
       "4          0.803238        0.005682                9            0.980000   \n",
       "5          0.804476        0.004904                4            0.978810   \n",
       "6          0.805571        0.006123                2            0.980357   \n",
       "7          0.803905        0.005401                7            0.981310   \n",
       "8          0.804190        0.004131                6            0.980238   \n",
       "9          0.803143        0.007208               10            0.981131   \n",
       "10         0.805857        0.007553                1            0.980893   \n",
       "11         0.804952        0.004056                3            0.981012   \n",
       "12         0.802905        0.003954               11            0.981190   \n",
       "13         0.803476        0.004420                8            0.979702   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0             0.979702            0.981190            0.980833   \n",
       "1             0.979405            0.980119            0.979702   \n",
       "2             0.982143            0.978988            0.980417   \n",
       "3             0.978988            0.979821            0.981905   \n",
       "4             0.979464            0.979524            0.981548   \n",
       "5             0.979702            0.980893            0.982560   \n",
       "6             0.980536            0.980655            0.981190   \n",
       "7             0.980238            0.979881            0.979940   \n",
       "8             0.979345            0.979286            0.981786   \n",
       "9             0.980595            0.980000            0.981131   \n",
       "10            0.979048            0.979464            0.979881   \n",
       "11            0.979821            0.980595            0.981667   \n",
       "12            0.978452            0.979821            0.980595   \n",
       "13            0.981250            0.978631            0.981607   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0             0.982143          0.980869         0.000805  \n",
       "1             0.979524          0.980060         0.000782  \n",
       "2             0.980655          0.980488         0.001008  \n",
       "3             0.982738          0.980786         0.001365  \n",
       "4             0.981369          0.980381         0.000901  \n",
       "5             0.981607          0.980714         0.001333  \n",
       "6             0.980179          0.980583         0.000344  \n",
       "7             0.979345          0.980143         0.000651  \n",
       "8             0.978750          0.979881         0.001065  \n",
       "9             0.979821          0.980536         0.000549  \n",
       "10            0.981369          0.980131         0.000871  \n",
       "11            0.979464          0.980512         0.000796  \n",
       "12            0.980595          0.980131         0.000945  \n",
       "13            0.980238          0.980286         0.001073  \n",
       "\n",
       "[14 rows x 21 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, random_state=True, shuffle=True)\n",
    "params = {'max_features': range(1, 15)}\n",
    "gs_model4 = GridSearchCV(rf_model,\n",
    "                         param_grid=params,\n",
    "                         verbose=1,\n",
    "                         n_jobs=-1,\n",
    "                         scoring='accuracy',\n",
    "                         return_train_score=True,\n",
    "                         cv=folds)\n",
    "gs_model4.fit(x_train, y_train)\n",
    "results = pd.DataFrame(gs_model4.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(results['param_max_features'], results['mean_train_score'], label='Training accuracy')\n",
    "plt.plot(results['param_max_features'], results['mean_test_score'], label='Test accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameter with including all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   48.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed: 15.2min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.335103</td>\n",
       "      <td>0.031292</td>\n",
       "      <td>0.011953</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804952</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>422</td>\n",
       "      <td>0.804583</td>\n",
       "      <td>0.803155</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>0.808512</td>\n",
       "      <td>0.805298</td>\n",
       "      <td>0.805452</td>\n",
       "      <td>0.001760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.552301</td>\n",
       "      <td>0.015293</td>\n",
       "      <td>0.019304</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805095</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>420</td>\n",
       "      <td>0.805536</td>\n",
       "      <td>0.804762</td>\n",
       "      <td>0.807321</td>\n",
       "      <td>0.809702</td>\n",
       "      <td>0.804643</td>\n",
       "      <td>0.806393</td>\n",
       "      <td>0.001912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.856315</td>\n",
       "      <td>0.029375</td>\n",
       "      <td>0.028433</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805857</td>\n",
       "      <td>0.006168</td>\n",
       "      <td>412</td>\n",
       "      <td>0.802500</td>\n",
       "      <td>0.806488</td>\n",
       "      <td>0.805536</td>\n",
       "      <td>0.813393</td>\n",
       "      <td>0.806726</td>\n",
       "      <td>0.806929</td>\n",
       "      <td>0.003566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.295781</td>\n",
       "      <td>0.024103</td>\n",
       "      <td>0.012164</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807810</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>375</td>\n",
       "      <td>0.804940</td>\n",
       "      <td>0.809643</td>\n",
       "      <td>0.811190</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.809940</td>\n",
       "      <td>0.809048</td>\n",
       "      <td>0.002137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.635249</td>\n",
       "      <td>0.040867</td>\n",
       "      <td>0.018576</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.007958</td>\n",
       "      <td>390</td>\n",
       "      <td>0.807798</td>\n",
       "      <td>0.809345</td>\n",
       "      <td>0.806190</td>\n",
       "      <td>0.808452</td>\n",
       "      <td>0.808690</td>\n",
       "      <td>0.808095</td>\n",
       "      <td>0.001073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.963374</td>\n",
       "      <td>0.075161</td>\n",
       "      <td>0.048753</td>\n",
       "      <td>0.012233</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807571</td>\n",
       "      <td>0.006379</td>\n",
       "      <td>379</td>\n",
       "      <td>0.804643</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>0.810893</td>\n",
       "      <td>0.811607</td>\n",
       "      <td>0.807798</td>\n",
       "      <td>0.808131</td>\n",
       "      <td>0.002751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.362452</td>\n",
       "      <td>0.023946</td>\n",
       "      <td>0.012026</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806857</td>\n",
       "      <td>0.008062</td>\n",
       "      <td>392</td>\n",
       "      <td>0.809345</td>\n",
       "      <td>0.808988</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.809345</td>\n",
       "      <td>0.807202</td>\n",
       "      <td>0.808143</td>\n",
       "      <td>0.001402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.728958</td>\n",
       "      <td>0.044047</td>\n",
       "      <td>0.044232</td>\n",
       "      <td>0.021359</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806286</td>\n",
       "      <td>0.007529</td>\n",
       "      <td>402</td>\n",
       "      <td>0.803929</td>\n",
       "      <td>0.808750</td>\n",
       "      <td>0.804821</td>\n",
       "      <td>0.807976</td>\n",
       "      <td>0.807679</td>\n",
       "      <td>0.806631</td>\n",
       "      <td>0.001896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.978153</td>\n",
       "      <td>0.059971</td>\n",
       "      <td>0.031262</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806190</td>\n",
       "      <td>0.007109</td>\n",
       "      <td>403</td>\n",
       "      <td>0.804643</td>\n",
       "      <td>0.807143</td>\n",
       "      <td>0.806131</td>\n",
       "      <td>0.810179</td>\n",
       "      <td>0.806190</td>\n",
       "      <td>0.806857</td>\n",
       "      <td>0.001843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.301345</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.011562</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>125</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804667</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>426</td>\n",
       "      <td>0.803452</td>\n",
       "      <td>0.808631</td>\n",
       "      <td>0.802500</td>\n",
       "      <td>0.812560</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.807333</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.601351</td>\n",
       "      <td>0.013701</td>\n",
       "      <td>0.019096</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>125</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807190</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>385</td>\n",
       "      <td>0.803036</td>\n",
       "      <td>0.804345</td>\n",
       "      <td>0.807798</td>\n",
       "      <td>0.808036</td>\n",
       "      <td>0.810655</td>\n",
       "      <td>0.806774</td>\n",
       "      <td>0.002741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.034542</td>\n",
       "      <td>0.031543</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>125</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>409</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.807440</td>\n",
       "      <td>0.812738</td>\n",
       "      <td>0.805893</td>\n",
       "      <td>0.807357</td>\n",
       "      <td>0.002806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.315801</td>\n",
       "      <td>0.023103</td>\n",
       "      <td>0.013804</td>\n",
       "      <td>0.005541</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>409</td>\n",
       "      <td>0.802560</td>\n",
       "      <td>0.803810</td>\n",
       "      <td>0.803810</td>\n",
       "      <td>0.807321</td>\n",
       "      <td>0.809643</td>\n",
       "      <td>0.805429</td>\n",
       "      <td>0.002639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.719065</td>\n",
       "      <td>0.046583</td>\n",
       "      <td>0.027342</td>\n",
       "      <td>0.009133</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804952</td>\n",
       "      <td>0.007790</td>\n",
       "      <td>422</td>\n",
       "      <td>0.803869</td>\n",
       "      <td>0.805179</td>\n",
       "      <td>0.800893</td>\n",
       "      <td>0.807381</td>\n",
       "      <td>0.807440</td>\n",
       "      <td>0.804952</td>\n",
       "      <td>0.002441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.226187</td>\n",
       "      <td>0.172628</td>\n",
       "      <td>0.038138</td>\n",
       "      <td>0.006340</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803095</td>\n",
       "      <td>0.006904</td>\n",
       "      <td>430</td>\n",
       "      <td>0.801964</td>\n",
       "      <td>0.804524</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.807560</td>\n",
       "      <td>0.804286</td>\n",
       "      <td>0.804500</td>\n",
       "      <td>0.001786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.417802</td>\n",
       "      <td>0.082150</td>\n",
       "      <td>0.013147</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>75</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805381</td>\n",
       "      <td>0.009813</td>\n",
       "      <td>418</td>\n",
       "      <td>0.804762</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.806786</td>\n",
       "      <td>0.807560</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.806155</td>\n",
       "      <td>0.001087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.703416</td>\n",
       "      <td>0.079448</td>\n",
       "      <td>0.023189</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804476</td>\n",
       "      <td>0.007336</td>\n",
       "      <td>428</td>\n",
       "      <td>0.802917</td>\n",
       "      <td>0.804702</td>\n",
       "      <td>0.803095</td>\n",
       "      <td>0.807440</td>\n",
       "      <td>0.809226</td>\n",
       "      <td>0.805476</td>\n",
       "      <td>0.002480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.727665</td>\n",
       "      <td>0.079886</td>\n",
       "      <td>0.064238</td>\n",
       "      <td>0.016251</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>75</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804524</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>427</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.808036</td>\n",
       "      <td>0.803869</td>\n",
       "      <td>0.807381</td>\n",
       "      <td>0.806012</td>\n",
       "      <td>0.806310</td>\n",
       "      <td>0.001427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.594545</td>\n",
       "      <td>0.072337</td>\n",
       "      <td>0.017264</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806143</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>405</td>\n",
       "      <td>0.807321</td>\n",
       "      <td>0.803036</td>\n",
       "      <td>0.804702</td>\n",
       "      <td>0.810595</td>\n",
       "      <td>0.805119</td>\n",
       "      <td>0.806155</td>\n",
       "      <td>0.002607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.084820</td>\n",
       "      <td>0.087412</td>\n",
       "      <td>0.058880</td>\n",
       "      <td>0.032151</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.008712</td>\n",
       "      <td>396</td>\n",
       "      <td>0.805179</td>\n",
       "      <td>0.804821</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.808452</td>\n",
       "      <td>0.806012</td>\n",
       "      <td>0.806560</td>\n",
       "      <td>0.001546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.824711</td>\n",
       "      <td>0.131585</td>\n",
       "      <td>0.061509</td>\n",
       "      <td>0.017952</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802619</td>\n",
       "      <td>0.007598</td>\n",
       "      <td>431</td>\n",
       "      <td>0.803036</td>\n",
       "      <td>0.805179</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.808214</td>\n",
       "      <td>0.804762</td>\n",
       "      <td>0.803821</td>\n",
       "      <td>0.003391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.581069</td>\n",
       "      <td>0.058237</td>\n",
       "      <td>0.016465</td>\n",
       "      <td>0.006589</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>125</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804857</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>424</td>\n",
       "      <td>0.800952</td>\n",
       "      <td>0.806310</td>\n",
       "      <td>0.808274</td>\n",
       "      <td>0.811071</td>\n",
       "      <td>0.805417</td>\n",
       "      <td>0.806405</td>\n",
       "      <td>0.003346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.930201</td>\n",
       "      <td>0.043668</td>\n",
       "      <td>0.038886</td>\n",
       "      <td>0.009685</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>125</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806143</td>\n",
       "      <td>0.008175</td>\n",
       "      <td>405</td>\n",
       "      <td>0.804107</td>\n",
       "      <td>0.803036</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.809762</td>\n",
       "      <td>0.806905</td>\n",
       "      <td>0.806095</td>\n",
       "      <td>0.002355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.191321</td>\n",
       "      <td>0.073792</td>\n",
       "      <td>0.036244</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>125</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 4, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804810</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>425</td>\n",
       "      <td>0.800655</td>\n",
       "      <td>0.804048</td>\n",
       "      <td>0.804940</td>\n",
       "      <td>0.808571</td>\n",
       "      <td>0.803750</td>\n",
       "      <td>0.804393</td>\n",
       "      <td>0.002540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.434820</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.013010</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 5, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811333</td>\n",
       "      <td>0.008351</td>\n",
       "      <td>263</td>\n",
       "      <td>0.817917</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.810595</td>\n",
       "      <td>0.814226</td>\n",
       "      <td>0.809881</td>\n",
       "      <td>0.812524</td>\n",
       "      <td>0.003132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.876229</td>\n",
       "      <td>0.045821</td>\n",
       "      <td>0.025121</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 5, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809619</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>321</td>\n",
       "      <td>0.808095</td>\n",
       "      <td>0.807738</td>\n",
       "      <td>0.810833</td>\n",
       "      <td>0.813810</td>\n",
       "      <td>0.809405</td>\n",
       "      <td>0.809976</td>\n",
       "      <td>0.002205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.352511</td>\n",
       "      <td>0.028881</td>\n",
       "      <td>0.036384</td>\n",
       "      <td>0.006449</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 5, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809048</td>\n",
       "      <td>0.005832</td>\n",
       "      <td>338</td>\n",
       "      <td>0.807381</td>\n",
       "      <td>0.808571</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.810774</td>\n",
       "      <td>0.809298</td>\n",
       "      <td>0.002486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.476210</td>\n",
       "      <td>0.036528</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 5, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806190</td>\n",
       "      <td>0.007588</td>\n",
       "      <td>403</td>\n",
       "      <td>0.804107</td>\n",
       "      <td>0.810595</td>\n",
       "      <td>0.806964</td>\n",
       "      <td>0.809286</td>\n",
       "      <td>0.806012</td>\n",
       "      <td>0.807393</td>\n",
       "      <td>0.002312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.839667</td>\n",
       "      <td>0.016825</td>\n",
       "      <td>0.024427</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 5, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808095</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>369</td>\n",
       "      <td>0.805179</td>\n",
       "      <td>0.806905</td>\n",
       "      <td>0.807500</td>\n",
       "      <td>0.812738</td>\n",
       "      <td>0.811845</td>\n",
       "      <td>0.808833</td>\n",
       "      <td>0.002938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.317916</td>\n",
       "      <td>0.040763</td>\n",
       "      <td>0.057073</td>\n",
       "      <td>0.025514</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 5, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809905</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>308</td>\n",
       "      <td>0.808810</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.808690</td>\n",
       "      <td>0.812738</td>\n",
       "      <td>0.809940</td>\n",
       "      <td>0.809702</td>\n",
       "      <td>0.001610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.893782</td>\n",
       "      <td>0.020334</td>\n",
       "      <td>0.014226</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 8, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>231</td>\n",
       "      <td>0.817202</td>\n",
       "      <td>0.815774</td>\n",
       "      <td>0.818155</td>\n",
       "      <td>0.813274</td>\n",
       "      <td>0.808512</td>\n",
       "      <td>0.814583</td>\n",
       "      <td>0.003453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1.672980</td>\n",
       "      <td>0.076746</td>\n",
       "      <td>0.026346</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 8, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816571</td>\n",
       "      <td>0.005487</td>\n",
       "      <td>81</td>\n",
       "      <td>0.817679</td>\n",
       "      <td>0.815298</td>\n",
       "      <td>0.816964</td>\n",
       "      <td>0.820417</td>\n",
       "      <td>0.818036</td>\n",
       "      <td>0.817679</td>\n",
       "      <td>0.001662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>2.567117</td>\n",
       "      <td>0.079378</td>\n",
       "      <td>0.038677</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 8, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814810</td>\n",
       "      <td>0.006464</td>\n",
       "      <td>151</td>\n",
       "      <td>0.817917</td>\n",
       "      <td>0.816607</td>\n",
       "      <td>0.817976</td>\n",
       "      <td>0.820893</td>\n",
       "      <td>0.815119</td>\n",
       "      <td>0.817702</td>\n",
       "      <td>0.001906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.926278</td>\n",
       "      <td>0.051292</td>\n",
       "      <td>0.014116</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>300</td>\n",
       "      <td>125</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 8, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815143</td>\n",
       "      <td>0.005276</td>\n",
       "      <td>140</td>\n",
       "      <td>0.817560</td>\n",
       "      <td>0.815238</td>\n",
       "      <td>0.817798</td>\n",
       "      <td>0.821250</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.817369</td>\n",
       "      <td>0.002255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1.721339</td>\n",
       "      <td>0.033838</td>\n",
       "      <td>0.026344</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>300</td>\n",
       "      <td>125</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 8, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816143</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>99</td>\n",
       "      <td>0.817440</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.818452</td>\n",
       "      <td>0.819405</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.817488</td>\n",
       "      <td>0.001732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>2.622965</td>\n",
       "      <td>0.056731</td>\n",
       "      <td>0.038982</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>300</td>\n",
       "      <td>125</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 8, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815762</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>115</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.817619</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.821190</td>\n",
       "      <td>0.812202</td>\n",
       "      <td>0.817274</td>\n",
       "      <td>0.002883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1.048559</td>\n",
       "      <td>0.031489</td>\n",
       "      <td>0.015011</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816524</td>\n",
       "      <td>0.006598</td>\n",
       "      <td>82</td>\n",
       "      <td>0.818214</td>\n",
       "      <td>0.818571</td>\n",
       "      <td>0.818988</td>\n",
       "      <td>0.820357</td>\n",
       "      <td>0.819821</td>\n",
       "      <td>0.819190</td>\n",
       "      <td>0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>2.229029</td>\n",
       "      <td>0.050654</td>\n",
       "      <td>0.028424</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817667</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>14</td>\n",
       "      <td>0.818571</td>\n",
       "      <td>0.818571</td>\n",
       "      <td>0.819405</td>\n",
       "      <td>0.822738</td>\n",
       "      <td>0.819940</td>\n",
       "      <td>0.819845</td>\n",
       "      <td>0.001537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>3.114138</td>\n",
       "      <td>0.047685</td>\n",
       "      <td>0.038193</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816905</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>64</td>\n",
       "      <td>0.818036</td>\n",
       "      <td>0.817976</td>\n",
       "      <td>0.818571</td>\n",
       "      <td>0.822024</td>\n",
       "      <td>0.820655</td>\n",
       "      <td>0.819452</td>\n",
       "      <td>0.001614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1.030713</td>\n",
       "      <td>0.041236</td>\n",
       "      <td>0.016079</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817476</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>26</td>\n",
       "      <td>0.818452</td>\n",
       "      <td>0.819821</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.821964</td>\n",
       "      <td>0.819345</td>\n",
       "      <td>0.819488</td>\n",
       "      <td>0.001414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>2.053482</td>\n",
       "      <td>0.031867</td>\n",
       "      <td>0.034081</td>\n",
       "      <td>0.009001</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817190</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>47</td>\n",
       "      <td>0.817917</td>\n",
       "      <td>0.819464</td>\n",
       "      <td>0.818155</td>\n",
       "      <td>0.823393</td>\n",
       "      <td>0.819821</td>\n",
       "      <td>0.819750</td>\n",
       "      <td>0.001963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3.151584</td>\n",
       "      <td>0.027873</td>\n",
       "      <td>0.041156</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.006009</td>\n",
       "      <td>61</td>\n",
       "      <td>0.817976</td>\n",
       "      <td>0.818333</td>\n",
       "      <td>0.818512</td>\n",
       "      <td>0.822440</td>\n",
       "      <td>0.820060</td>\n",
       "      <td>0.819464</td>\n",
       "      <td>0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1.035092</td>\n",
       "      <td>0.037168</td>\n",
       "      <td>0.014779</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816619</td>\n",
       "      <td>0.006202</td>\n",
       "      <td>79</td>\n",
       "      <td>0.818571</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.818810</td>\n",
       "      <td>0.821607</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.819548</td>\n",
       "      <td>0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>2.057614</td>\n",
       "      <td>0.121739</td>\n",
       "      <td>0.024024</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816238</td>\n",
       "      <td>0.006037</td>\n",
       "      <td>95</td>\n",
       "      <td>0.818810</td>\n",
       "      <td>0.817560</td>\n",
       "      <td>0.817798</td>\n",
       "      <td>0.821905</td>\n",
       "      <td>0.819583</td>\n",
       "      <td>0.819131</td>\n",
       "      <td>0.001565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3.244229</td>\n",
       "      <td>0.104562</td>\n",
       "      <td>0.037436</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817524</td>\n",
       "      <td>0.005781</td>\n",
       "      <td>21</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.818571</td>\n",
       "      <td>0.818452</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.819762</td>\n",
       "      <td>0.819536</td>\n",
       "      <td>0.001383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1.022296</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.015888</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>125</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>6</td>\n",
       "      <td>0.818274</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.818393</td>\n",
       "      <td>0.823214</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.819583</td>\n",
       "      <td>0.001849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>2.082414</td>\n",
       "      <td>0.037952</td>\n",
       "      <td>0.029712</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>125</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816810</td>\n",
       "      <td>0.005684</td>\n",
       "      <td>70</td>\n",
       "      <td>0.817619</td>\n",
       "      <td>0.818333</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.821786</td>\n",
       "      <td>0.819583</td>\n",
       "      <td>0.819214</td>\n",
       "      <td>0.001434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>3.169616</td>\n",
       "      <td>0.072497</td>\n",
       "      <td>0.041977</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>125</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817238</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>45</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.818869</td>\n",
       "      <td>0.818869</td>\n",
       "      <td>0.822976</td>\n",
       "      <td>0.820179</td>\n",
       "      <td>0.819750</td>\n",
       "      <td>0.001774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>1.027098</td>\n",
       "      <td>0.019449</td>\n",
       "      <td>0.019136</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817286</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>41</td>\n",
       "      <td>0.817262</td>\n",
       "      <td>0.816369</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.821607</td>\n",
       "      <td>0.818036</td>\n",
       "      <td>0.818155</td>\n",
       "      <td>0.001808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>1.962269</td>\n",
       "      <td>0.023956</td>\n",
       "      <td>0.030244</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>3</td>\n",
       "      <td>0.817083</td>\n",
       "      <td>0.817679</td>\n",
       "      <td>0.818393</td>\n",
       "      <td>0.821488</td>\n",
       "      <td>0.818155</td>\n",
       "      <td>0.818560</td>\n",
       "      <td>0.001531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>2.821516</td>\n",
       "      <td>0.078881</td>\n",
       "      <td>0.039171</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816381</td>\n",
       "      <td>0.005659</td>\n",
       "      <td>88</td>\n",
       "      <td>0.818214</td>\n",
       "      <td>0.815893</td>\n",
       "      <td>0.817679</td>\n",
       "      <td>0.821131</td>\n",
       "      <td>0.817976</td>\n",
       "      <td>0.818179</td>\n",
       "      <td>0.001687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.937614</td>\n",
       "      <td>0.042146</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>75</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816429</td>\n",
       "      <td>0.005820</td>\n",
       "      <td>87</td>\n",
       "      <td>0.815774</td>\n",
       "      <td>0.817262</td>\n",
       "      <td>0.815893</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.818512</td>\n",
       "      <td>0.817345</td>\n",
       "      <td>0.001394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>1.978662</td>\n",
       "      <td>0.024844</td>\n",
       "      <td>0.027434</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814857</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>149</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.818095</td>\n",
       "      <td>0.819643</td>\n",
       "      <td>0.812857</td>\n",
       "      <td>0.817167</td>\n",
       "      <td>0.002284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>2.943247</td>\n",
       "      <td>0.057356</td>\n",
       "      <td>0.038987</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>75</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815714</td>\n",
       "      <td>0.005957</td>\n",
       "      <td>116</td>\n",
       "      <td>0.817679</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.818036</td>\n",
       "      <td>0.819643</td>\n",
       "      <td>0.815714</td>\n",
       "      <td>0.817690</td>\n",
       "      <td>0.001260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.951760</td>\n",
       "      <td>0.034699</td>\n",
       "      <td>0.016026</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815905</td>\n",
       "      <td>0.006078</td>\n",
       "      <td>108</td>\n",
       "      <td>0.817202</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.816964</td>\n",
       "      <td>0.821548</td>\n",
       "      <td>0.814048</td>\n",
       "      <td>0.817452</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>1.888037</td>\n",
       "      <td>0.045799</td>\n",
       "      <td>0.026206</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816333</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>89</td>\n",
       "      <td>0.817202</td>\n",
       "      <td>0.816905</td>\n",
       "      <td>0.817738</td>\n",
       "      <td>0.821190</td>\n",
       "      <td>0.815298</td>\n",
       "      <td>0.817667</td>\n",
       "      <td>0.001941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>2.881286</td>\n",
       "      <td>0.102214</td>\n",
       "      <td>0.040499</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>53</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.817976</td>\n",
       "      <td>0.821310</td>\n",
       "      <td>0.818274</td>\n",
       "      <td>0.818369</td>\n",
       "      <td>0.001537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>0.978310</td>\n",
       "      <td>0.047459</td>\n",
       "      <td>0.014456</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>125</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814619</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>156</td>\n",
       "      <td>0.814107</td>\n",
       "      <td>0.813214</td>\n",
       "      <td>0.816369</td>\n",
       "      <td>0.820238</td>\n",
       "      <td>0.817619</td>\n",
       "      <td>0.816310</td>\n",
       "      <td>0.002513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>1.919084</td>\n",
       "      <td>0.029779</td>\n",
       "      <td>0.030812</td>\n",
       "      <td>0.007773</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>125</td>\n",
       "      <td>40</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817048</td>\n",
       "      <td>0.006386</td>\n",
       "      <td>58</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.817202</td>\n",
       "      <td>0.817679</td>\n",
       "      <td>0.819643</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.000906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>2.775324</td>\n",
       "      <td>0.348386</td>\n",
       "      <td>0.032029</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>125</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 9, 'min_sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816857</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>66</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.816845</td>\n",
       "      <td>0.818869</td>\n",
       "      <td>0.821250</td>\n",
       "      <td>0.817381</td>\n",
       "      <td>0.818369</td>\n",
       "      <td>0.001588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>432 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         0.335103      0.031292         0.011953        0.001996   \n",
       "1         0.552301      0.015293         0.019304        0.002747   \n",
       "2         0.856315      0.029375         0.028433        0.002098   \n",
       "3         0.295781      0.024103         0.012164        0.002516   \n",
       "4         0.635249      0.040867         0.018576        0.002320   \n",
       "5         0.963374      0.075161         0.048753        0.012233   \n",
       "6         0.362452      0.023946         0.012026        0.002287   \n",
       "7         0.728958      0.044047         0.044232        0.021359   \n",
       "8         0.978153      0.059971         0.031262        0.004818   \n",
       "9         0.301345      0.017172         0.011562        0.001811   \n",
       "10        0.601351      0.013701         0.019096        0.001531   \n",
       "11        0.913043      0.034542         0.031543        0.005681   \n",
       "12        0.315801      0.023103         0.013804        0.005541   \n",
       "13        0.719065      0.046583         0.027342        0.009133   \n",
       "14        1.226187      0.172628         0.038138        0.006340   \n",
       "15        0.417802      0.082150         0.013147        0.001846   \n",
       "16        0.703416      0.079448         0.023189        0.003688   \n",
       "17        1.727665      0.079886         0.064238        0.016251   \n",
       "18        0.594545      0.072337         0.017264        0.005232   \n",
       "19        1.084820      0.087412         0.058880        0.032151   \n",
       "20        1.824711      0.131585         0.061509        0.017952   \n",
       "21        0.581069      0.058237         0.016465        0.006589   \n",
       "22        0.930201      0.043668         0.038886        0.009685   \n",
       "23        1.191321      0.073792         0.036244        0.005588   \n",
       "24        0.434820      0.012359         0.013010        0.001911   \n",
       "25        0.876229      0.045821         0.025121        0.003953   \n",
       "26        1.352511      0.028881         0.036384        0.006449   \n",
       "27        0.476210      0.036528         0.013479        0.003836   \n",
       "28        0.839667      0.016825         0.024427        0.003668   \n",
       "29        1.317916      0.040763         0.057073        0.025514   \n",
       "..             ...           ...              ...             ...   \n",
       "402       0.893782      0.020334         0.014226        0.000999   \n",
       "403       1.672980      0.076746         0.026346        0.002447   \n",
       "404       2.567117      0.079378         0.038677        0.001945   \n",
       "405       0.926278      0.051292         0.014116        0.002103   \n",
       "406       1.721339      0.033838         0.026344        0.001142   \n",
       "407       2.622965      0.056731         0.038982        0.002740   \n",
       "408       1.048559      0.031489         0.015011        0.001670   \n",
       "409       2.229029      0.050654         0.028424        0.001807   \n",
       "410       3.114138      0.047685         0.038193        0.003112   \n",
       "411       1.030713      0.041236         0.016079        0.002358   \n",
       "412       2.053482      0.031867         0.034081        0.009001   \n",
       "413       3.151584      0.027873         0.041156        0.004907   \n",
       "414       1.035092      0.037168         0.014779        0.001529   \n",
       "415       2.057614      0.121739         0.024024        0.002087   \n",
       "416       3.244229      0.104562         0.037436        0.002656   \n",
       "417       1.022296      0.022461         0.015888        0.002455   \n",
       "418       2.082414      0.037952         0.029712        0.003548   \n",
       "419       3.169616      0.072497         0.041977        0.004255   \n",
       "420       1.027098      0.019449         0.019136        0.003739   \n",
       "421       1.962269      0.023956         0.030244        0.001374   \n",
       "422       2.821516      0.078881         0.039171        0.003667   \n",
       "423       0.937614      0.042146         0.015550        0.004338   \n",
       "424       1.978662      0.024844         0.027434        0.000746   \n",
       "425       2.943247      0.057356         0.038987        0.003741   \n",
       "426       0.951760      0.034699         0.016026        0.002047   \n",
       "427       1.888037      0.045799         0.026206        0.002138   \n",
       "428       2.881286      0.102214         0.040499        0.002372   \n",
       "429       0.978310      0.047459         0.014456        0.001488   \n",
       "430       1.919084      0.029779         0.030812        0.007773   \n",
       "431       2.775324      0.348386         0.032029        0.006014   \n",
       "\n",
       "    param_max_depth param_max_features param_min_samples_leaf  \\\n",
       "0                 3                  4                    100   \n",
       "1                 3                  4                    100   \n",
       "2                 3                  4                    100   \n",
       "3                 3                  4                    100   \n",
       "4                 3                  4                    100   \n",
       "5                 3                  4                    100   \n",
       "6                 3                  4                    100   \n",
       "7                 3                  4                    100   \n",
       "8                 3                  4                    100   \n",
       "9                 3                  4                    100   \n",
       "10                3                  4                    100   \n",
       "11                3                  4                    100   \n",
       "12                3                  4                    300   \n",
       "13                3                  4                    300   \n",
       "14                3                  4                    300   \n",
       "15                3                  4                    300   \n",
       "16                3                  4                    300   \n",
       "17                3                  4                    300   \n",
       "18                3                  4                    300   \n",
       "19                3                  4                    300   \n",
       "20                3                  4                    300   \n",
       "21                3                  4                    300   \n",
       "22                3                  4                    300   \n",
       "23                3                  4                    300   \n",
       "24                3                  5                    100   \n",
       "25                3                  5                    100   \n",
       "26                3                  5                    100   \n",
       "27                3                  5                    100   \n",
       "28                3                  5                    100   \n",
       "29                3                  5                    100   \n",
       "..              ...                ...                    ...   \n",
       "402               5                  8                    300   \n",
       "403               5                  8                    300   \n",
       "404               5                  8                    300   \n",
       "405               5                  8                    300   \n",
       "406               5                  8                    300   \n",
       "407               5                  8                    300   \n",
       "408               5                  9                    100   \n",
       "409               5                  9                    100   \n",
       "410               5                  9                    100   \n",
       "411               5                  9                    100   \n",
       "412               5                  9                    100   \n",
       "413               5                  9                    100   \n",
       "414               5                  9                    100   \n",
       "415               5                  9                    100   \n",
       "416               5                  9                    100   \n",
       "417               5                  9                    100   \n",
       "418               5                  9                    100   \n",
       "419               5                  9                    100   \n",
       "420               5                  9                    300   \n",
       "421               5                  9                    300   \n",
       "422               5                  9                    300   \n",
       "423               5                  9                    300   \n",
       "424               5                  9                    300   \n",
       "425               5                  9                    300   \n",
       "426               5                  9                    300   \n",
       "427               5                  9                    300   \n",
       "428               5                  9                    300   \n",
       "429               5                  9                    300   \n",
       "430               5                  9                    300   \n",
       "431               5                  9                    300   \n",
       "\n",
       "    param_min_samples_split param_n_estimators  \\\n",
       "0                        50                 20   \n",
       "1                        50                 40   \n",
       "2                        50                 60   \n",
       "3                        75                 20   \n",
       "4                        75                 40   \n",
       "5                        75                 60   \n",
       "6                       100                 20   \n",
       "7                       100                 40   \n",
       "8                       100                 60   \n",
       "9                       125                 20   \n",
       "10                      125                 40   \n",
       "11                      125                 60   \n",
       "12                       50                 20   \n",
       "13                       50                 40   \n",
       "14                       50                 60   \n",
       "15                       75                 20   \n",
       "16                       75                 40   \n",
       "17                       75                 60   \n",
       "18                      100                 20   \n",
       "19                      100                 40   \n",
       "20                      100                 60   \n",
       "21                      125                 20   \n",
       "22                      125                 40   \n",
       "23                      125                 60   \n",
       "24                       50                 20   \n",
       "25                       50                 40   \n",
       "26                       50                 60   \n",
       "27                       75                 20   \n",
       "28                       75                 40   \n",
       "29                       75                 60   \n",
       "..                      ...                ...   \n",
       "402                     100                 20   \n",
       "403                     100                 40   \n",
       "404                     100                 60   \n",
       "405                     125                 20   \n",
       "406                     125                 40   \n",
       "407                     125                 60   \n",
       "408                      50                 20   \n",
       "409                      50                 40   \n",
       "410                      50                 60   \n",
       "411                      75                 20   \n",
       "412                      75                 40   \n",
       "413                      75                 60   \n",
       "414                     100                 20   \n",
       "415                     100                 40   \n",
       "416                     100                 60   \n",
       "417                     125                 20   \n",
       "418                     125                 40   \n",
       "419                     125                 60   \n",
       "420                      50                 20   \n",
       "421                      50                 40   \n",
       "422                      50                 60   \n",
       "423                      75                 20   \n",
       "424                      75                 40   \n",
       "425                      75                 60   \n",
       "426                     100                 20   \n",
       "427                     100                 40   \n",
       "428                     100                 60   \n",
       "429                     125                 20   \n",
       "430                     125                 40   \n",
       "431                     125                 60   \n",
       "\n",
       "                                                params  ...  mean_test_score  \\\n",
       "0    {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.804952   \n",
       "1    {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.805095   \n",
       "2    {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.805857   \n",
       "3    {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.807810   \n",
       "4    {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.807000   \n",
       "5    {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.807571   \n",
       "6    {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.806857   \n",
       "7    {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.806286   \n",
       "8    {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.806190   \n",
       "9    {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.804667   \n",
       "10   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.807190   \n",
       "11   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.805952   \n",
       "12   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.805952   \n",
       "13   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.804952   \n",
       "14   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.803095   \n",
       "15   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.805381   \n",
       "16   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.804476   \n",
       "17   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.804524   \n",
       "18   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.806143   \n",
       "19   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.806667   \n",
       "20   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.802619   \n",
       "21   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.804857   \n",
       "22   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.806143   \n",
       "23   {'max_depth': 3, 'max_features': 4, 'min_sampl...  ...         0.804810   \n",
       "24   {'max_depth': 3, 'max_features': 5, 'min_sampl...  ...         0.811333   \n",
       "25   {'max_depth': 3, 'max_features': 5, 'min_sampl...  ...         0.809619   \n",
       "26   {'max_depth': 3, 'max_features': 5, 'min_sampl...  ...         0.809048   \n",
       "27   {'max_depth': 3, 'max_features': 5, 'min_sampl...  ...         0.806190   \n",
       "28   {'max_depth': 3, 'max_features': 5, 'min_sampl...  ...         0.808095   \n",
       "29   {'max_depth': 3, 'max_features': 5, 'min_sampl...  ...         0.809905   \n",
       "..                                                 ...  ...              ...   \n",
       "402  {'max_depth': 5, 'max_features': 8, 'min_sampl...  ...         0.812143   \n",
       "403  {'max_depth': 5, 'max_features': 8, 'min_sampl...  ...         0.816571   \n",
       "404  {'max_depth': 5, 'max_features': 8, 'min_sampl...  ...         0.814810   \n",
       "405  {'max_depth': 5, 'max_features': 8, 'min_sampl...  ...         0.815143   \n",
       "406  {'max_depth': 5, 'max_features': 8, 'min_sampl...  ...         0.816143   \n",
       "407  {'max_depth': 5, 'max_features': 8, 'min_sampl...  ...         0.815762   \n",
       "408  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.816524   \n",
       "409  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.817667   \n",
       "410  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.816905   \n",
       "411  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.817476   \n",
       "412  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.817190   \n",
       "413  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.817000   \n",
       "414  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.816619   \n",
       "415  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.816238   \n",
       "416  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.817524   \n",
       "417  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.817857   \n",
       "418  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.816810   \n",
       "419  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.817238   \n",
       "420  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.817286   \n",
       "421  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.818000   \n",
       "422  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.816381   \n",
       "423  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.816429   \n",
       "424  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.814857   \n",
       "425  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.815714   \n",
       "426  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.815905   \n",
       "427  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.816333   \n",
       "428  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.817143   \n",
       "429  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.814619   \n",
       "430  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.817048   \n",
       "431  {'max_depth': 5, 'max_features': 9, 'min_sampl...  ...         0.816857   \n",
       "\n",
       "     std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0          0.007054              422            0.804583            0.803155   \n",
       "1          0.008879              420            0.805536            0.804762   \n",
       "2          0.006168              412            0.802500            0.806488   \n",
       "3          0.007925              375            0.804940            0.809643   \n",
       "4          0.007958              390            0.807798            0.809345   \n",
       "5          0.006379              379            0.804643            0.805714   \n",
       "6          0.008062              392            0.809345            0.808988   \n",
       "7          0.007529              402            0.803929            0.808750   \n",
       "8          0.007109              403            0.804643            0.807143   \n",
       "9          0.006522              426            0.803452            0.808631   \n",
       "10         0.007478              385            0.803036            0.804345   \n",
       "11         0.006624              409            0.805714            0.805000   \n",
       "12         0.006115              409            0.802560            0.803810   \n",
       "13         0.007790              422            0.803869            0.805179   \n",
       "14         0.006904              430            0.801964            0.804524   \n",
       "15         0.009813              418            0.804762            0.806667   \n",
       "16         0.007336              428            0.802917            0.804702   \n",
       "17         0.008426              427            0.806250            0.808036   \n",
       "18         0.007139              405            0.807321            0.803036   \n",
       "19         0.008712              396            0.805179            0.804821   \n",
       "20         0.007598              431            0.803036            0.805179   \n",
       "21         0.007163              424            0.800952            0.806310   \n",
       "22         0.008175              405            0.804107            0.803036   \n",
       "23         0.007148              425            0.800655            0.804048   \n",
       "24         0.008351              263            0.817917            0.810000   \n",
       "25         0.006478              321            0.808095            0.807738   \n",
       "26         0.005832              338            0.807381            0.808571   \n",
       "27         0.007588              403            0.804107            0.810595   \n",
       "28         0.006662              369            0.805179            0.806905   \n",
       "29         0.006957              308            0.808810            0.808333   \n",
       "..              ...              ...                 ...                 ...   \n",
       "402        0.009848              231            0.817202            0.815774   \n",
       "403        0.005487               81            0.817679            0.815298   \n",
       "404        0.006464              151            0.817917            0.816607   \n",
       "405        0.005276              140            0.817560            0.815238   \n",
       "406        0.005176               99            0.817440            0.814286   \n",
       "407        0.005587              115            0.817500            0.817619   \n",
       "408        0.006598               82            0.818214            0.818571   \n",
       "409        0.005556               14            0.818571            0.818571   \n",
       "410        0.005292               64            0.818036            0.817976   \n",
       "411        0.005642               26            0.818452            0.819821   \n",
       "412        0.005428               47            0.817917            0.819464   \n",
       "413        0.006009               61            0.817976            0.818333   \n",
       "414        0.006202               79            0.818571            0.818750   \n",
       "415        0.006037               95            0.818810            0.817560   \n",
       "416        0.005781               21            0.818750            0.818571   \n",
       "417        0.005518                6            0.818274            0.818750   \n",
       "418        0.005684               70            0.817619            0.818333   \n",
       "419        0.006270               45            0.817857            0.818869   \n",
       "420        0.006572               41            0.817262            0.816369   \n",
       "421        0.005573                3            0.817083            0.817679   \n",
       "422        0.005659               88            0.818214            0.815893   \n",
       "423        0.005820               87            0.815774            0.817262   \n",
       "424        0.006717              149            0.817857            0.817381   \n",
       "425        0.005957              116            0.817679            0.817381   \n",
       "426        0.006078              108            0.817202            0.817500   \n",
       "427        0.006450               89            0.817202            0.816905   \n",
       "428        0.006252               53            0.817143            0.817143   \n",
       "429        0.006360              156            0.814107            0.813214   \n",
       "430        0.006386               58            0.817381            0.817202   \n",
       "431        0.006022               66            0.817500            0.816845   \n",
       "\n",
       "     split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0              0.805714            0.808512            0.805298   \n",
       "1              0.807321            0.809702            0.804643   \n",
       "2              0.805536            0.813393            0.806726   \n",
       "3              0.811190            0.809524            0.809940   \n",
       "4              0.806190            0.808452            0.808690   \n",
       "5              0.810893            0.811607            0.807798   \n",
       "6              0.805833            0.809345            0.807202   \n",
       "7              0.804821            0.807976            0.807679   \n",
       "8              0.806131            0.810179            0.806190   \n",
       "9              0.802500            0.812560            0.809524   \n",
       "10             0.807798            0.808036            0.810655   \n",
       "11             0.807440            0.812738            0.805893   \n",
       "12             0.803810            0.807321            0.809643   \n",
       "13             0.800893            0.807381            0.807440   \n",
       "14             0.804167            0.807560            0.804286   \n",
       "15             0.806786            0.807560            0.805000   \n",
       "16             0.803095            0.807440            0.809226   \n",
       "17             0.803869            0.807381            0.806012   \n",
       "18             0.804702            0.810595            0.805119   \n",
       "19             0.808333            0.808452            0.806012   \n",
       "20             0.797917            0.808214            0.804762   \n",
       "21             0.808274            0.811071            0.805417   \n",
       "22             0.806667            0.809762            0.806905   \n",
       "23             0.804940            0.808571            0.803750   \n",
       "24             0.810595            0.814226            0.809881   \n",
       "25             0.810833            0.813810            0.809405   \n",
       "26             0.806429            0.813333            0.810774   \n",
       "27             0.806964            0.809286            0.806012   \n",
       "28             0.807500            0.812738            0.811845   \n",
       "29             0.808690            0.812738            0.809940   \n",
       "..                  ...                 ...                 ...   \n",
       "402            0.818155            0.813274            0.808512   \n",
       "403            0.816964            0.820417            0.818036   \n",
       "404            0.817976            0.820893            0.815119   \n",
       "405            0.817798            0.821250            0.815000   \n",
       "406            0.818452            0.819405            0.817857   \n",
       "407            0.817857            0.821190            0.812202   \n",
       "408            0.818988            0.820357            0.819821   \n",
       "409            0.819405            0.822738            0.819940   \n",
       "410            0.818571            0.822024            0.820655   \n",
       "411            0.817857            0.821964            0.819345   \n",
       "412            0.818155            0.823393            0.819821   \n",
       "413            0.818512            0.822440            0.820060   \n",
       "414            0.818810            0.821607            0.820000   \n",
       "415            0.817798            0.821905            0.819583   \n",
       "416            0.818452            0.822143            0.819762   \n",
       "417            0.818393            0.823214            0.819286   \n",
       "418            0.818750            0.821786            0.819583   \n",
       "419            0.818869            0.822976            0.820179   \n",
       "420            0.817500            0.821607            0.818036   \n",
       "421            0.818393            0.821488            0.818155   \n",
       "422            0.817679            0.821131            0.817976   \n",
       "423            0.815893            0.819286            0.818512   \n",
       "424            0.818095            0.819643            0.812857   \n",
       "425            0.818036            0.819643            0.815714   \n",
       "426            0.816964            0.821548            0.814048   \n",
       "427            0.817738            0.821190            0.815298   \n",
       "428            0.817976            0.821310            0.818274   \n",
       "429            0.816369            0.820238            0.817619   \n",
       "430            0.817679            0.819643            0.817381   \n",
       "431            0.818869            0.821250            0.817381   \n",
       "\n",
       "     mean_train_score  std_train_score  \n",
       "0            0.805452         0.001760  \n",
       "1            0.806393         0.001912  \n",
       "2            0.806929         0.003566  \n",
       "3            0.809048         0.002137  \n",
       "4            0.808095         0.001073  \n",
       "5            0.808131         0.002751  \n",
       "6            0.808143         0.001402  \n",
       "7            0.806631         0.001896  \n",
       "8            0.806857         0.001843  \n",
       "9            0.807333         0.003800  \n",
       "10           0.806774         0.002741  \n",
       "11           0.807357         0.002806  \n",
       "12           0.805429         0.002639  \n",
       "13           0.804952         0.002441  \n",
       "14           0.804500         0.001786  \n",
       "15           0.806155         0.001087  \n",
       "16           0.805476         0.002480  \n",
       "17           0.806310         0.001427  \n",
       "18           0.806155         0.002607  \n",
       "19           0.806560         0.001546  \n",
       "20           0.803821         0.003391  \n",
       "21           0.806405         0.003346  \n",
       "22           0.806095         0.002355  \n",
       "23           0.804393         0.002540  \n",
       "24           0.812524         0.003132  \n",
       "25           0.809976         0.002205  \n",
       "26           0.809298         0.002486  \n",
       "27           0.807393         0.002312  \n",
       "28           0.808833         0.002938  \n",
       "29           0.809702         0.001610  \n",
       "..                ...              ...  \n",
       "402          0.814583         0.003453  \n",
       "403          0.817679         0.001662  \n",
       "404          0.817702         0.001906  \n",
       "405          0.817369         0.002255  \n",
       "406          0.817488         0.001732  \n",
       "407          0.817274         0.002883  \n",
       "408          0.819190         0.000792  \n",
       "409          0.819845         0.001537  \n",
       "410          0.819452         0.001614  \n",
       "411          0.819488         0.001414  \n",
       "412          0.819750         0.001963  \n",
       "413          0.819464         0.001650  \n",
       "414          0.819548         0.001147  \n",
       "415          0.819131         0.001565  \n",
       "416          0.819536         0.001383  \n",
       "417          0.819583         0.001849  \n",
       "418          0.819214         0.001434  \n",
       "419          0.819750         0.001774  \n",
       "420          0.818155         0.001808  \n",
       "421          0.818560         0.001531  \n",
       "422          0.818179         0.001687  \n",
       "423          0.817345         0.001394  \n",
       "424          0.817167         0.002284  \n",
       "425          0.817690         0.001260  \n",
       "426          0.817452         0.002394  \n",
       "427          0.817667         0.001941  \n",
       "428          0.818369         0.001537  \n",
       "429          0.816310         0.002513  \n",
       "430          0.817857         0.000906  \n",
       "431          0.818369         0.001588  \n",
       "\n",
       "[432 rows x 25 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, random_state=True, shuffle=True)\n",
    "params = {\n",
    "    'max_features': range(4, 10),\n",
    "    'n_estimators': range(20, 80, 20),\n",
    "    'min_samples_leaf': range(100, 400, 200),\n",
    "    'min_samples_split': range(50, 150, 25),\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "gs_model4 = GridSearchCV(rf_model,\n",
    "                         param_grid=params,\n",
    "                         verbose=1,\n",
    "                         n_jobs=-1,\n",
    "                         scoring='accuracy',\n",
    "                         return_train_score=True,\n",
    "                         cv=folds)\n",
    "gs_model4.fit(x_train, y_train)\n",
    "results = pd.DataFrame(gs_model4.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4,\n",
       " 'max_features': 8,\n",
       " 'min_samples_leaf': 100,\n",
       " 'min_samples_split': 100,\n",
       " 'n_estimators': 60}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features=8, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=100, min_samples_split=100,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "rfc = RandomForestClassifier(bootstrap=True,\n",
    "                             max_depth=4,\n",
    "                             min_samples_leaf=100, \n",
    "                             min_samples_split=100,\n",
    "                             max_features=8,\n",
    "                             n_estimators=60)\n",
    "rfc.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90      7058\n",
      "           1       0.69      0.35      0.47      1942\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      9000\n",
      "   macro avg       0.77      0.65      0.68      9000\n",
      "weighted avg       0.81      0.83      0.80      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8104444444444444"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rfc1 = RandomForestClassifier(bootstrap=True,\n",
    "                             max_depth=10,\n",
    "                             min_samples_leaf=100, \n",
    "                             min_samples_split=200,\n",
    "                             max_features=10,\n",
    "                             n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features=10, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=100, min_samples_split=200,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90      7058\n",
      "           1       0.70      0.34      0.46      1942\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      9000\n",
      "   macro avg       0.77      0.65      0.68      9000\n",
      "weighted avg       0.81      0.83      0.80      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = rfc.predict(x_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6774  284]\n",
      " [1274  668]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8104444444444444"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-18T12:51:27.943Z"
    }
   },
   "outputs": [],
   "source": [
    "import fancyimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
